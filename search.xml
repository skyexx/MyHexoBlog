<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>关于工作</title>
    <url>/%E5%85%B3%E4%BA%8E%E5%B7%A5%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>job</tag>
      </tags>
  </entry>
  <entry>
    <title>自我剖析</title>
    <url>/%E8%87%AA%E6%88%91%E5%89%96%E6%9E%90/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>job</tag>
      </tags>
  </entry>
  <entry>
    <title>面对焦虑</title>
    <url>/%E6%9C%89%E5%AF%AB%E5%B0%B1%E5%A5%BD%EF%BD%9C%E9%9D%A2%E5%B0%8D%E7%84%A6%E6%85%AE/</url>
    <content><![CDATA[<p>感觉周围最近抱怨「大环境」的言论甚嚣尘上，自己也难免受此影响，有些焦虑。焦虑的时候读书总不会错的，这里纪录一些最近看到的段落：</p>
<p><strong>《宇宙》里卡尔萨根在回顾人类发展史的时候说：</strong></p>
<blockquote>
<p>「让我们回望过去。数不清的年月前，潮起潮落的滩涂里，生命逐渐成形。他挣扎着变成一个又一个不同的形状，攫取了一种又一种不同的力量，终于自信地爬上陆地。经过一代又一代的变化，他控制了天空，也潜入了黑暗的深渊；我们看着他在愤怒和饥饿中重塑自身,看着他越来越像我们。他不断伸展，不断优化，向着难以置信的目标一刻不停地前进。然后，他变成了我们，生命的韵律至今在我们的大脑和动脉中搏动……」</p>
</blockquote>
<p><strong>美国最高法院首席大法官约翰·罗伯茨(John Roberts)，参加儿子的初中毕业典礼时的演讲：</strong></p>
<p>From time to time in the years to come, I hope you will be treated unfairly, so that you will come to know the value of justice.<br>我希望在未来岁月中，你能时不时地遭遇不公，唯有如此，你才能懂得公正的价值。</p>
<p>I hope that you will suffer betrayal because that will teach you the importance of loyalty.<br>我希望你尝到背叛的滋味，这样你才能领悟到忠诚之重要。</p>
<p>Sorry to say, butI hope you will be lonely from time to time so that you don’t take friends for granted.<br>抱歉，我还希望你们时常感到孤独，唯有如此，你才 不会视朋友为理所当然。</p>
<p>I wish you bad luck,again, from time to time so that you will be conscious of the role of chance in life and understand that your success is not completely deserved and that the failure of others is not completely deserved either.<br>我祝你们偶尔运气不佳，这样你才会意识到机遇在人 生中扮演的⻆色，从而明白你的成功并非天经地义， 而他人的失败也不是命中注定。</p>
<p>And when you lose, as you will from time to time, I hope every now and then, your opponent will gloat over your failure. It is a way for you to understand the importance of sportsmanship.<br>当你偶尔失败时，我愿你的对手时不时地会幸灾乐 祸。这样你才能懂得互相尊重的竞技精神的重要。</p>
<p>I hope you’ll be ignoredso you know the importance of listening to others.<br>我希望你被人无视，唯有如此，你才懂得倾听他人有多重要。</p>
<p>And I hope you will have just enough painto learn compassion.<br>我祝你感受足够的痛楚来学会同情。</p>
<p>Unless you are perfect, it does not mean don’t make any changes.<br>除非你完美无瑕，那么这句话绝不意味着不去改变自己。</p>
<p>“The unexamined life is not worth living.”<br>未经自省的人生没有意义。</p>
]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title>说清楚，讲明白</title>
    <url>/%E6%9C%89%E5%AF%AB%E5%B0%B1%E5%A5%BD%EF%BD%9C%E8%AA%AA%E6%B8%85%E6%A5%9A%EF%BC%8C%E8%AC%9B%E6%98%8E%E7%99%BD/</url>
    <content><![CDATA[<p>最近发生了几件小事，正好应和题目，就此记录一下。</p>
<p>「把话说清楚」是表达最基本的要求，但想做好，其实并不简单。上周「系统分析与设计」课堂报告，要求不多，就是前三周的上课内容。但大多数组别的报告，都没有让老师满意，问题出在「这节课不需要你们把系统设计的很复杂，重点是要说清楚、讲明白」。</p>
<p>什么叫「说清楚、讲明白」？用这节课来举例就是：从「需求分析」到「需求塑模」，每一步骤都需要按部就班，先说清楚每个步骤需要怎样的方法和规则，需要用到哪些工具来呈现（活动图、使用个案图等），我们又是怎么依照这些规则，用这些工具来完成我们的设计。要逻辑清楚，重点明确。</p>
<p>另一件事发生在导师的课堂作业，9组有8组被退回重写，读了这么多年书，这种情况也是头一回。作为助教，把作业发给老师之前，我其实都会大概看一下每组的内容，这次的要求是分析一家公司的数位策略，粗略看过之后觉得都还不错呀，实在想不明白为什么都要退回重写。</p>
<p>解铃还须系铃人「大家太爱写模棱两可的答案，回答全是『都可以』。但是分析和做决策的时候，一定要找到一两个关键人事物，也就是80/20法则中最重要的那20%。要杜绝『囫囵吞枣』式的做事、做决策的方式，事实上不存在『都可以』这个选项，任何选择都是有代价的，但做决策的时候要习惯想清楚、选一边，然后坚决的『踩下去』。即使选错边也没有关系，可以再反省、检讨。但是如果一开始就都选，或者都不选的话，你做对了什么，做错了什么，是完全弄不清楚的。」我也算是又上了一课，也反思自己在「想清楚、讲清楚、写清楚」这些几个方面都有太多需要精进的，而且时常会受到惰性的蛊惑，陷入「差不多得了」的想法，以后要杜绝。</p>
<p>最后，分享轻松一点的，王兴以前的blog中写道「据说，把想法写下来有助于思考，因为只有想清楚的事情才能写得出来。所以，我正努力多写。」看来大佬有时也困于「想清楚，写明白」，多少安慰到了我。</p>
]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>reflection</tag>
      </tags>
  </entry>
  <entry>
    <title>说人话</title>
    <url>/%E6%9C%89%E5%AF%AB%E5%B0%B1%E5%A5%BD%EF%BD%9C%E8%AA%AA%E4%BA%BA%E8%A9%B1/</url>
    <content><![CDATA[<p>前段时间写文章，总觉得不得要领。</p>
<p>论述的文章还算好写，想清楚论点，说明白论据，有一定逻辑，基本都是能看的。但是，稍微有点描述性，需要表达情感的文章就麻烦了，写的狗屁不通是常有的事情。以前看到一篇逻辑清晰，语言简洁的文章不觉得怎样。现在来看，只能感慨其功底的深厚。</p>
<p>不过，写的渐渐多了之后，虽然精进不多，但也算有了一点心得体会。</p>
<p>首先，感动笔。当你有写文章需求的时候，甭管以前写的有多烂，有多怕写文章，都要感快动笔写起来。就像系统开发里常说的 prototype，你先写出个雏型，再慢慢修改。不管是你自己读，还是拿给别人修改，都是一种反馈，有了反馈再去做修正。修正的多了，就算是垃圾堆，最后也能修出个像模像样的东西来。怕的就是开始顾虑太多，然后一拖再推，最后信心被拖垮了，就更写不出来了。</p>
<p>其次，说人话。这一点真的很难做到，我经常就是写着写着，句子不自觉就拗口起来，一句话恨不得，要转折个五六次，才说到重点。避免这个问题，有个简单的办法 —— 善用逗号。这是我在读导师写的文章时，发现的一个小诀窍。她写文章很少出现文绉绉的长句，每句话都不长，旨在把内容表达清楚，而不追求引经据典、炫耀文笔。所以读起来清新流畅，十分舒服。后来，我也模仿这个方式，尽量每句话说的精简到位，避免用冗长、重复的句子表达。多练几次，真的会通顺不少。</p>
<p>不过，想要把文章写好，写的流畅，让人读起来舒服，还是唯手熟尔。所以多写啊！</p>
]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>reflection</tag>
      </tags>
  </entry>
  <entry>
    <title>重新开始写作</title>
    <url>/%E6%9C%89%E5%AF%AB%E5%B0%B1%E5%A5%BD%EF%BD%9C%E9%87%8D%E6%96%B0%E9%96%8B%E5%A7%8B%E5%AF%AB%E4%BD%9C/</url>
    <content><![CDATA[<p>忙忙碌碌的2022上半年就这么倏忽而过，最近决定重新开始写一写文章。</p>
<p>其一，因为近期在写论文的过程中，明显感觉自己组织长篇文章的能力实在太差。<br>其二，也想要好好审视一下自己、纪录一下生活。同时也是希望可以慢慢培养出写作的习惯。</p>
<p>最近在听池大的 podcast<a href="https://app.podcastguru.io/podcast/mactalk-1619054900" target="_blank" rel="noopener">「MacTalk·夜航西飞」</a>，里面有一期和孟岩的聊天，讲到二人坚持写作这件事情，很多人会质疑「你创业工作这么忙了，怎么会还有自己的时间写东西」。大家一方面，惊讶于二人的时间管理，另一方面不理解坚持写作的意义。</p>
<p>孟岩就提到芒格曾说的一个故事：</p>
<blockquote>
<p>当我在给一只黑猩猩讲解一件事情的时候，最终会留下什么呢？</p>
<p>一头雾水的黑猩猩，和一个更理解这件事情的我自己。</p>
</blockquote>
<p>写作的意义是梳理，是reset的过程。有些事情我们以为自己了解了，但当需要给别人讲解的时候，又变得十分困难。时常没办法非常迅速的组织出一套清晰且有逻辑的说词。本质上还是我们对于这件事的理解不够深入，把「知道了」误以为「理解了」。</p>
<p>写作的好处就在于，可以帮助你再一次的梳理。写的过程就是重新思考，重新构建的过程。就像第一遍，你凭感觉搭了一个乐高城堡，但发现怪怪的。第二遍，就看着说明书，把各个部分又重新搭建了一遍。写作的过程就像是第二遍看说明，只不过这个说明书要你自己总结。</p>
]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>reflection</tag>
      </tags>
  </entry>
  <entry>
    <title>分类算法实作 Titanic disaster dataset</title>
    <url>/Titanicdataset/</url>
    <content><![CDATA[<p>完整链接：<a href="https://rpubs.com/SiQingYe/751255" target="_blank" rel="noopener">https://rpubs.com/SiQingYe/751255</a></p>
<h2 id="资料处理流程"><a href="#资料处理流程" class="headerlink" title="资料处理流程"></a>资料处理流程</h2><ol>
<li>资料前处理<ul>
<li>简单的Feature Engineering<ul>
<li>只保留Cabin的舱位号（前面的字母）。</li>
<li>把Name中的有一定含义的 title 元素提取出来，并将比较少用的title合并到比较常用的tittle中，建立一个新的类别“Title”</li>
<li>把姓氏提取出来，创建新的类别“Surname”</li>
</ul>
</li>
<li>Missing Data<ul>
<li>NA值 &amp; 空白值</li>
</ul>
</li>
<li>减少资料量<ul>
<li>属性的筛选：删掉不要的属性</li>
</ul>
</li>
<li>正规化处理</li>
</ul>
</li>
<li>模型的建立<ul>
<li>随机森林（Random Forest）</li>
<li>SVM（Support Vector Machines）</li>
<li>GBM（Gradient Boosting Machine）</li>
</ul>
</li>
<li>模型的解释</li>
<li>预测及分析<ul>
<li>混淆矩阵</li>
<li>ROC</li>
<li>AUC</li>
</ul>
</li>
</ol>
<h2 id="资料前处理"><a href="#资料前处理" class="headerlink" title="资料前处理"></a>资料前处理</h2><h3 id="简单的Feature-Engineering："><a href="#简单的Feature-Engineering：" class="headerlink" title="简单的Feature Engineering："></a>简单的Feature Engineering：</h3><ul>
<li>只保留Cabin的舱位号（前面的字母）。</li>
<li>把Name中的有一定含义的 title 元素提取出来，并将比较少用的title合并到比较常用的tittle中，建立一个新的类别“Title”</li>
<li>把姓氏提取出来，创建新的类别“Surname”<br><img src="https://img-blog.csdnimg.cn/20210630161713992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h3 id="Missing-data："><a href="#Missing-data：" class="headerlink" title="Missing data："></a>Missing data：</h3></li>
</ul>
<p>NA值主要来自Age和Cabin（Survived的缺失值，是test中填补的NA）</p>
<p><img src="https://img-blog.csdnimg.cn/20210630162043986.png#pic_center" alt="在这里插入图片描述"></p>
<pre><code>方法：这边我们选择用mice填补Age和Fare的缺失值

查看填补之后的结果：</code></pre><p><img src="https://img-blog.csdnimg.cn/20210630162117560.png" alt="在这里插入图片描述"></p>
<p>空值：只来自Embarked</p>
<p>  <img src="https://img-blog.csdnimg.cn/20210630162218181.png" alt="在这里插入图片描述"></p>
<pre><code>查看Embarked的类别及各个类别的资料笔数，选择资料笔数最的类别（“S”）填补填补到Embarked的空值中</code></pre><p><img src="https://img-blog.csdnimg.cn/20210630162440826.png" alt="在这里插入图片描述"></p>
<h3 id="减少资料量："><a href="#减少资料量：" class="headerlink" title="减少资料量："></a>减少资料量：</h3><p>属性的筛选：删掉不要的属性</p>
<ul>
<li>Name &amp; Surname：类别太多了，并且没有什么特别的用途。</li>
<li>Ticket：里面都是一些随机的数字，没有特多含义，并且是多值属性不好处理。</li>
<li>Cabin：有太多的NA值，并且为类别变数不好填充，如果给一个Ncabin的类别，会使得属性非常unbalance。</li>
<li>PassengerId：只是一个序列号，没有太多的含义。 <img src="https://img-blog.csdnimg.cn/2021063016251447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h3 id="Data-正规化："><a href="#Data-正规化：" class="headerlink" title="Data 正规化："></a>Data 正规化：</h3></li>
</ul>
<p>数值变量的range没有特别的大，所以没有做特别的正规化处理</p>
<h2 id="模型的建立"><a href="#模型的建立" class="headerlink" title="模型的建立"></a>模型的建立</h2><p>因为test 资料集中没有“Survuved”栏位，所以我们从train资料集中分出30%作为验证集。</p>
<p>剩下的train中的70%资料用于训练模型，训练中也会做cross-validation。</p>
<p>下面建立了三个模型：</p>
<ul>
<li>随机森林（Random Forest）</li>
<li>SVM（Support Vector Machines）</li>
<li>GBM（Gradient Boosting Machine）</li>
<li>⚠️ KNN一般可以作为分类的baseline</li>
</ul>
<h3 id="随机森林："><a href="#随机森林：" class="headerlink" title="随机森林："></a>随机森林：</h3><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林</span></span><br><span class="line">set.seed(<span class="number">100</span>)</span><br><span class="line">titanic_rf &lt;- train(factor(Survived) ~ ., data=train_data, method=<span class="string">'rf'</span>, </span><br><span class="line">										trControl=trainControl(method=<span class="string">"cv"</span>, number=<span class="number">5</span>))</span><br><span class="line">titanic_rf</span><br><span class="line">plot(titanic_rf)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20210630162556173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们可以看出选择出的最好的模型是mtry = 3的时候。</p>
<h3 id="SVM："><a href="#SVM：" class="headerlink" title="SVM："></a>SVM：</h3><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SVM</span></span><br><span class="line">set.seed(<span class="number">100</span>)</span><br><span class="line">titanic_svm &lt;- train(factor(Survived) ~., data=SVMtrain_data, method=<span class="string">'svmRadial'</span>, preProcess= c(<span class="string">'center'</span>, <span class="string">'scale'</span>), </span><br><span class="line">                     trControl= trainControl(method=<span class="string">"cv"</span>, number=<span class="number">5</span>, classProbs = <span class="literal">T</span>))</span><br><span class="line">titanic_svm</span><br><span class="line">plot(titanic_svm</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20210630162643796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们可以看出最好的模型是 sigma=0.2828，C=0.25的时候。</p>
<h3 id="GBM："><a href="#GBM：" class="headerlink" title="GBM："></a>GBM：</h3><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (GBM) model</span></span><br><span class="line">set.seed(<span class="number">100</span>)</span><br><span class="line">titanic_gbm &lt;- train(factor(Survived) ~., data=train_data, method=<span class="string">'gbm'</span>, preProcess= c(<span class="string">'center'</span>, <span class="string">'scale'</span>), </span><br><span class="line">                     trControl=trainControl(method=<span class="string">"cv"</span>, number=<span class="number">7</span>), verbose=<span class="literal">FALSE</span>)</span><br><span class="line">print(titanic_gbm)</span><br><span class="line">plot(titanic_gbm)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20210630162715791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们可以看出GBM选出的最好的模型是n.trees=200，且深度为3 interaction.depth = 3的模型。</p>
<h2 id="模型的解释"><a href="#模型的解释" class="headerlink" title="模型的解释"></a>模型的解释</h2><p>这里直接利用<code>library(&quot;DALEX&quot;)</code>包的解释函数对三个模型进行解释性分析</p>
<h3 id="累积残差分布："><a href="#累积残差分布：" class="headerlink" title="累积残差分布："></a>累积残差分布：</h3><p><img src="https://img-blog.csdnimg.cn/20210630162745616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>由上图我们可以看，绿色的线在最上方，也就是SVM中大部分的样本残差都比较大。而红色的线是RF模型，它的大部分的样本残差都比较小。可以看出树的模型的残差线对于SVM这类的模型都要来的比较小一些。</p>
<h3 id="变数重要性分析："><a href="#变数重要性分析：" class="headerlink" title="变数重要性分析："></a>变数重要性分析：</h3><p><img src="https://img-blog.csdnimg.cn/20210630162825232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出三个模型中的变数重要性的排序基本上是一样的。</p>
<h2 id="测试训练好的模型并分析结果"><a href="#测试训练好的模型并分析结果" class="headerlink" title="测试训练好的模型并分析结果"></a>测试训练好的模型并分析结果</h2><p>我们用之前从train的资料集中分出的valid资料来做各个模型的测试</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用不同的已经训练好的模型分类预测：</span></span><br><span class="line">rf_probs = predict(titanic_rf,valid_feature,type = <span class="string">"prob"</span>) <span class="comment"># 训练集中的预测情形</span></span><br><span class="line">svm_probs = predict(titanic_svm,valid_feature,type = <span class="string">"prob"</span>)</span><br><span class="line">gbm_probs = predict(titanic_gbm,valid_feature,type = <span class="string">"prob"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="预测出来的结果（这边仅以Random-Forrest的处理过程为例）："><a href="#预测出来的结果（这边仅以Random-Forrest的处理过程为例）：" class="headerlink" title="预测出来的结果（这边仅以Random Forrest的处理过程为例）："></a>预测出来的结果（这边仅以Random Forrest的处理过程为例）：</h3><p>如图：<br><img src="https://img-blog.csdnimg.cn/20210630162927787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看训练集的预测情形</span></span><br><span class="line">rf_summaryvalid &lt;- rf_validDataPred %&gt;% </span><br><span class="line">  filter(yes &gt; <span class="number">0.5</span>) %&gt;%                           <span class="comment"># yes 为预测存活下来的机率</span></span><br><span class="line">  summarise(count = n(), </span><br><span class="line">            accuracy_rate = mean(Survived))              <span class="comment">#  统计预测存活下来的人数与实际比较的准确率</span></span><br><span class="line"></span><br><span class="line">rf_summaryvalid</span><br><span class="line"><span class="comment"># count  accuracy_rate</span></span><br><span class="line"><span class="comment"># 106	 0.6981132</span></span><br></pre></td></tr></table></figure>

<p>我们将是否存活的机率的门槛设置为0.5，我们可以看到计算出来的准确率为：0.6981132</p>
<p>我们想了解不同门槛值的预测情况，所以就先粗略切了[0,0.5,0.55,0.6,0.65,0.7,0.8,1]这些区间来看看，是否有更好的门槛值设定。</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看测试集分类机率在不同门槛值下的预测情况</span></span><br><span class="line">rf_summaryvalidCut &lt;- rf_validDataPred %&gt;% </span><br><span class="line">  <span class="comment"># 看不同的分类机率做区间查看其准确率</span></span><br><span class="line">  mutate(interval = cut(yes,breaks = c(<span class="number">0</span>,<span class="number">0.5</span>,<span class="number">0.55</span>,<span class="number">0.6</span>,<span class="number">0.65</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">1</span>),include.lowest = <span class="literal">T</span>)) %&gt;%  </span><br><span class="line">  group_by(interval) %&gt;%</span><br><span class="line">  summarise(count = n(), </span><br><span class="line">            accuracy_rate = mean(Survived))</span><br><span class="line"></span><br><span class="line">rf_summaryvalidCut</span><br></pre></td></tr></table></figure>

<p>根据下表的分布情况似乎切在0.8左右会更好<br><img src="https://img-blog.csdnimg.cn/20210630163015754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="混淆矩阵-："><a href="#混淆矩阵-：" class="headerlink" title="混淆矩阵 ："></a><strong>混淆矩阵 ：</strong></h3><p>那么我将预测结果依照分类机率大于0.8当成1来建立混淆矩阵，并对三个模型都建立了混淆矩阵来进行对比：<img src="https://img-blog.csdnimg.cn/20210630163057830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出三个模型：</p>
<ul>
<li><p>预测的准确率（Accuracy）以及F1 Score 最高的都是 Random Forrest</p>
</li>
<li><p>三个模型主要犯的都是FN（false negative）的错误，既将实际存活的人判断为了死亡。</p>
</li>
<li><p>同时我们可以看到SVM在准确率（Accuracy）上和其他两个模型相差的并不多，但是 F1 Score 的分数就要比其他两个模型低很多。</p>
<ul>
<li><p>Accuracy = (TP+TN)/(TP+FP+FN+TN)</p>
</li>
<li><p>Recall = TP/(TP+FN)</p>
</li>
<li><p>Precision = TP/(TP+FP)</p>
</li>
<li><p>F1-score = 2 * Precision * Recall / (Precision + Recall)</p>
<p>那么我们可以根据F1-score 的公式看出他是Precision和Recall两者调和平均，而Precision和Recall两者可以比较好的度量分类错误的情况。所以我们可以推测出SVM的模型对于FP和FN的判断不是很好，但对于TP和TN的预测效果还是可以的。</p>
</li>
</ul>
</li>
</ul>
<h3 id="ROC："><a href="#ROC：" class="headerlink" title="ROC："></a>ROC：</h3><p>上面的混淆矩阵我是透过比较粗糙的方式切出的 0.8 作为门槛来进行三个模型的比较的。</p>
<p>但是想进一步比较三个模型效果还是应该来看一个更为细致的ROC曲线和AUC的值。</p>
<p>红线为：Random Forest</p>
<ul>
<li>最佳的切分值：0.835 or 0.737</li>
</ul>
<p>绿线为：SVM</p>
<ul>
<li>最佳的切分值：0.808 or 0.798</li>
</ul>
<p>蓝线为：GBM</p>
<ul>
<li>最佳的切分值：0.885 or 0.717</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20210630163241918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在ROC的图中越是靠近左上角的部分他的TPR越大而FPR越小，也就是模型的效果就越好。可以看出蓝线和红线都完全包裹在绿线的外面，所以我们可以认为Random Forest和GBM做为这个资料集的分类器效果都比SVM更好。而Random Forest和GBM的比较我们我们只从ROC不能看出哪一个模型比较好，所以我们需要进一步分析两个模型的AUC值。</p>
<h3 id="AUC："><a href="#AUC：" class="headerlink" title="AUC："></a>AUC：</h3><p>AUC也就是ROC曲线下方的面积，AUC值越大的分类器，正确率越高。</p>
<p>根据 AUC 比较模型的效果也就是：Random Forest(0.844) &gt; GBM(0.836) &gt; SVM(0.795)</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>details</tag>
      </tags>
  </entry>
  <entry>
    <title>文字探勘-2｜断词以及tidy形式的转换</title>
    <url>/%E6%96%87%E5%AD%97%E6%8E%A2%E5%8B%98-2%EF%BD%9C%E6%96%B7%E8%A9%9E%E4%BB%A5%E5%8F%8Atidy%E5%BD%A2%E5%BC%8F%E7%9A%84%E8%BD%89%E6%8F%9B/</url>
    <content><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="涉及的知识点"><a href="#涉及的知识点" class="headerlink" title="涉及的知识点"></a>涉及的知识点</h2><ul>
<li>tidy形式的转换</li>
<li>断句</li>
<li>断词</li>
</ul>
<h2 id="会用到的packages"><a href="#会用到的packages" class="headerlink" title="会用到的packages"></a>会用到的packages</h2><p>会用到的packages和函数：</p>
<ul>
<li><p><strong>library(dplyr)</strong>：整合所有在前处理数据会常用的“逻辑”，加入pipeline的概念</p>
<ul>
<li>select()：挑选特定column出来</li>
<li>filter()：自订条件滤掉column中的资料</li>
<li>arrange()：调整row排列顺序</li>
<li>mutate()：以现有的column资料做运算，形成新的column</li>
<li>summarise()：将目前的资料做统计运算，形成统计结论</li>
<li>tibble()：将典型的字符向量变成 tidy 文本数据集</li>
</ul>
</li>
<li><p><strong>library(jiebaR)</strong>：用于断词（ref：<a href="https://zhuanlan.zhihu.com/p/35846130" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35846130</a> ）</p>
<ul>
<li>worker()：初始化断词引擎<ul>
<li>stop_word=”stop_word.txt”：停用词</li>
<li>user=”xxx.txt”：自定义词库：可以自己定义，也可以借用搜狗细胞词库，有大量专业领域词汇</li>
<li>bylines = TRUE：不保留标点符号</li>
<li>注意：user=”xxx.txt” 以及 stop_word=”stop_word.txt”的txt档案一定要是UTF-8编码的格式</li>
</ul>
</li>
<li>segment()：断词（一般配合supply来写成function使用）</li>
<li>filter_segment()：在前面worker()过滤后的基础上，再次过滤</li>
</ul>
</li>
<li><p><strong>library(tidytext)</strong>：</p>
<ul>
<li>unnest_tokens()：将文本拆分成tokens，转换成tidy的格式</li>
</ul>
</li>
<li><p><strong>library(tidyr)</strong>：重新定义资料框，留下绘图需要的点</p>
<ul>
<li>spread()：</li>
<li>gather()：</li>
<li>separate(): 将 info 栏位以分号分割成三个栏位，并且直接放进原 data frame 中</li>
</ul>
</li>
</ul>
<h1 id="实作"><a href="#实作" class="headerlink" title="实作"></a>实作</h1><h2 id="读取资料以及packages"><a href="#读取资料以及packages" class="headerlink" title="读取资料以及packages"></a>读取资料以及packages</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 避免中文乱码</span><br><span class="line">Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;zh_TW.UTF-8&quot;) </span><br><span class="line"></span><br><span class="line"># 载入资料</span><br><span class="line">library(dplyr)</span><br><span class="line">library(tidytext)</span><br><span class="line">library(wordcloud2)</span><br><span class="line">library(data.table)</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(wordcloud)</span><br><span class="line">library(tidyr)</span><br><span class="line">library(jiebaR)</span><br><span class="line"></span><br><span class="line"># 把文章和留言读进来</span><br><span class="line">MetaData = read.csv(&apos;./data/PTTcoin_articleMetaData.csv&apos;,encoding = &apos;UTF-8&apos;)</span><br><span class="line">Reviews  = read.csv(&apos;./data/PTTcoin_articleReviews.csv&apos;,encoding = &apos;UTF-8&apos;)</span><br><span class="line"></span><br><span class="line">MetaData$sentence &lt;- as.character(MetaData$sentence)</span><br><span class="line">Reviews$cmtContent &lt;- as.character(Reviews$cmtContent)</span><br><span class="line"></span><br><span class="line"># 挑选文章对应的留言</span><br><span class="line">Reviews = left_join(MetaData, Reviews[,c(&quot;artUrl&quot;, &quot;cmtContent&quot;)], by = &quot;artUrl&quot;)</span><br><span class="line"></span><br><span class="line"># 查看读进来的资料</span><br><span class="line">str(Reviews)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E8%AF%BB%E8%BF%9B%E6%9D%A5%E7%9A%84%E5%8E%9F%E5%A7%8B%E8%B5%84%E6%96%99.png?raw=true" alt="读进来的原始资料"> </p>
<h2 id="断词"><a href="#断词" class="headerlink" title="断词"></a>断词</h2><h3 id="初始化断词引擎"><a href="#初始化断词引擎" class="headerlink" title="初始化断词引擎"></a>初始化断词引擎</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 初始化断词引擎</span><br><span class="line">jieba_tokenizer &lt;- worker(stop_word = &quot;./stop_words.txt&quot;,user=&quot;./user_dict.txt&quot;)</span><br></pre></td></tr></table></figure>

<p>我们将设置好的断词引擎加入我们写好的断词函式中，方便在在我们资料格式中断词</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 自定义断词函式</span><br><span class="line">chinese_tokenizer &lt;- function(t) &#123;</span><br><span class="line">  lapply(t, function(x) &#123;</span><br><span class="line">    tokens &lt;- segment(x, jieba_tokenizer)</span><br><span class="line">    tokens &lt;- tokens[nchar(tokens)&gt;1]</span><br><span class="line">    return(tokens)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将原始资料断词</span><br><span class="line">tokens &lt;- MetaData %&gt;% </span><br><span class="line">  unnest_tokens(word, sentence, token=chinese_tokenizer)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 格式化日期栏位</span><br><span class="line">tokens$artDate &lt;- tokens$artDate %&gt;% as.Date(&quot;%Y/%m/%d&quot;)</span><br><span class="line"></span><br><span class="line"># 过滤特殊字元</span><br><span class="line">tokens &lt;- tokens %&gt;% </span><br><span class="line">  filter(!(word %in% stop_words)) %&gt;% # 去掉停用字里的一些词汇</span><br><span class="line">  filter(!grepl(&apos;[[:punct:]]&apos;,word)) %&gt;% # 去标点符号</span><br><span class="line">  filter(!grepl(&quot;[&apos;^0-9a-z&apos;]&quot;,word)) %&gt;% # 去英文、数字</span><br><span class="line">  filter(nchar(.$word)&gt;1) </span><br><span class="line">  # select(-artDate, -artUrl)</span><br><span class="line"></span><br><span class="line">head(tokens, 20)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E5%89%8D20%E7%9A%84tokens.png?raw=true" alt="查看出现词"> </p>
<h2 id="文字云"><a href="#文字云" class="headerlink" title="文字云"></a>文字云</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 文字云</span><br><span class="line"># 计算词汇的出现次数，如果词汇只有一个字则不列入计算</span><br><span class="line">tokens_count &lt;- tokens %&gt;% </span><br><span class="line">  filter(nchar(.$word)&gt;1) %&gt;%</span><br><span class="line">  group_by(word) %&gt;% </span><br><span class="line">  summarise(sum = n()) %&gt;% </span><br><span class="line">  filter(sum&gt;10) %&gt;%</span><br><span class="line">  arrange(desc(sum))</span><br><span class="line">head(tokens_count, 30)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E5%87%BA%E7%8E%B0%E6%9C%80%E5%A4%9A%E7%9A%84%E5%89%8D30%E4%B8%AA%E8%AF%8D.png?raw=true" alt="出现前30的词"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tokens_count %&gt;% </span><br><span class="line">  filter(word!=c(&quot;比特币&quot;,&quot;原文&quot;))%&gt;%</span><br><span class="line">#  wordcloud2()</span><br></pre></td></tr></table></figure>

<h2 id="词性的标注（NER）"><a href="#词性的标注（NER）" class="headerlink" title="词性的标注（NER）"></a>词性的标注（NER）</h2><p>那么如果我们想知道里面的名词都有哪些？</p>
<p>我们就需要更改worker()中type的参数，改为worker(type=”tag”)，就可以用jieba来标记中文的词性了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 初始化词性标注引擎</span><br><span class="line">tag_cuter &lt;- worker(type=&quot;tag&quot;, stop_word=&quot;./stop_words.txt&quot;,user=&quot;./user_dict.txt&quot;) </span><br><span class="line"></span><br><span class="line">raw_text &lt;- &quot;在狗狗币的市场中有一只巨鲸持有着30%的狗狗币，有一些线索指出这个持有者很有可能就是马斯克&quot;</span><br><span class="line">look &lt;- segment(raw_text, tag_cuter)</span><br><span class="line"></span><br><span class="line"># 标注词性的断词函式</span><br><span class="line">get_noun = function(x)&#123;</span><br><span class="line">  index = names(x) %in% c(&quot;n&quot;,&quot;nr&quot;,&quot;nr1&quot;,&quot;nr2&quot;,&quot;nrj&quot;,&quot;nrf&quot;,&quot;ns&quot;,&quot;nsf&quot;,&quot;nt&quot;,&quot;nz&quot;,&quot;nl&quot;,&quot;ng&quot;)</span><br><span class="line">  x[index]</span><br><span class="line">&#125;</span><br><span class="line">get_noun(look)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E8%AF%8D%E6%80%A7%E7%9A%84%E6%A0%87%E6%B3%A8.png?raw=true" alt="词性的标注"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 自定义断词函式</span><br><span class="line">tag_tokenizer &lt;- function(t) &#123;</span><br><span class="line">  lapply(t, function(x) &#123;</span><br><span class="line">    tokens &lt;- segment(x, tag_cuter)</span><br><span class="line">    tokens &lt;- tokens[nchar(tokens)&gt;1]</span><br><span class="line">    return(paste(tokens,names(tokens))) # 将词性和词合并在一个栏位中，后面会再分开</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将全文用标注词性的断词函式断开</span><br><span class="line">tag_tokens &lt;- MetaData %&gt;% </span><br><span class="line">  unnest_tokens(word, sentence, token=tag_tokenizer)</span><br><span class="line">str(tag_tokens)</span><br><span class="line"></span><br><span class="line"># 将词和词性分割成 2 栏，并新增至 Data Frame 中</span><br><span class="line">new_df &lt;- separate(tag_tokens, word, c(&quot;word&quot;, &quot;tag&quot;), &quot; &quot;)</span><br><span class="line"></span><br><span class="line"># 筛选出所有的名词</span><br><span class="line">new_df_n &lt;- new_df %&gt;%</span><br><span class="line">  filter(tag %in% c(&quot;n&quot;,&quot;nr&quot;,&quot;nr1&quot;,&quot;nr2&quot;,&quot;nrj&quot;,&quot;nrf&quot;,&quot;ns&quot;,&quot;nsf&quot;,&quot;nt&quot;,&quot;nz&quot;,&quot;nl&quot;,&quot;ng&quot;))</span><br><span class="line">  # filter(tag == &quot;x&quot;)</span><br><span class="line">head(new_df_n)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/%E7%AD%9B%E9%80%89%E5%87%BA%E7%9A%84%E6%89%80%E6%9C%89%E5%90%8D%E8%AF%8D.png?raw=true" alt="筛选出的所有名词"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 计算每个词出现的次数</span><br><span class="line">tokens_count &lt;- new_df_n %&gt;% </span><br><span class="line">  filter(nchar(.$word)&gt;1) %&gt;%</span><br><span class="line">  group_by(word) %&gt;% </span><br><span class="line">  summarise(sum = n()) %&gt;% </span><br><span class="line">  filter(sum&gt;10) %&gt;%</span><br><span class="line">  arrange(desc(sum))</span><br><span class="line"></span><br><span class="line"># 查看看出现最多的前20个名词</span><br><span class="line">head(tokens_count,20)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E6%9F%A5%E7%9C%8B%E5%90%8D%E8%AF%8D%E7%9A%84%E5%89%8D20.png?raw=true" alt="查看名词的前20"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 画出文字云</span><br><span class="line">tokens_count %&gt;% </span><br><span class="line">  filter(word!=c(&quot;比特币&quot;,&quot;价格&quot;))%&gt;%</span><br><span class="line">  top_n(20)%&gt;%</span><br><span class="line">  wordcloud2()</span><br><span class="line">  </span><br><span class="line"># “原文”是ptt中出现格式词汇，应该去掉。</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/R/SMA/2-%E6%96%87%E5%AD%97%E4%BA%91.png?raw=true" alt="文字云"></p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>courses</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>有关数据挖掘（Data mining）的一点反思</title>
    <url>/%E6%9C%89%E9%97%9C%20Data%20mining%20%E7%9A%84%E4%B8%80%E9%BB%9E%E5%8F%8D%E6%80%9D/</url>
    <content><![CDATA[<p>写这篇的起因，就是最近几节课课的期中报告都是Data mining相关的project。</p>
<h1 id="工具🔨-vs-目的🚩"><a href="#工具🔨-vs-目的🚩" class="headerlink" title="工具🔨 vs 目的🚩"></a>工具🔨 vs 目的🚩</h1><p>美国作家马克·吐温有句名言，说：“如果你身上唯一的工具是一把锤子，那么你会把所有的问题都看成钉子。”美国著名投资家查理·芒格，根据马克·吐温的这句话，将这种现象称为“拿锤子的人”——芒格分析说，人们经过年复一年的专业培训，会成为经济学家、工程师、营销经理、投资经理等等。一旦当他们了解并熟悉某一个领域的思维模式之后，他们就会到处尝试将所有遇到的问题，都用自己的专业思维模式来解决。</p>
<p>我们在学习的时候也会有这种问题，像是这学期的社群媒体分析的课上学习了一些nlp的知识以及工具，像是“情绪分析”，“关系网络”，“n-grams”等。就使得在分析某个议题的时候，我们就会很自然的直接套用这些分析工具，无论是否必要。</p>
<p>这就引出最近我在做 Data mining 的时候几点反思：</p>
<h2 id="我们分析的目的是什么？"><a href="#我们分析的目的是什么？" class="headerlink" title="我们分析的目的是什么？"></a>我们分析的目的是什么？</h2><p>很多时候我们甚至都还没明确分析的目的，只是有一个现成的资料集，就开始按照流程来套工具🔧。把一个资料集用额种各样工具处理完之后，再来从一堆结果中找有什么值得探讨和解释的points。这其实是一个很本末倒置的过程。我们的分析应该是为我们的目的服务的，而不是从结果中随意得出几个结论。</p>
<p>所以在开始一些列分析的步骤前一定要先厘清我们的目标：</p>
<ul>
<li>我们最主要分析的问题是什么？ （what？How？）</li>
<li>这个问题的合理性以及意义所在？</li>
<li>围绕这个问题我们能从那些方面展开讨论？</li>
</ul>
<p>我们也可以借助一些工具来帮助我们展开我们要分析的问题，厘清我们要分析的目标：</p>
<ul>
<li>问题树</li>
<li>⋯⋯</li>
</ul>
<p>总之多花些时间在厘清分析的目的会便于我们后续运用更合适的工具，得到我们想要的结果。不会做着做着突然突然迷失在一堆资料中，开始怀疑自己到底在干嘛。</p>
<h2 id="工具是否合适？"><a href="#工具是否合适？" class="headerlink" title="工具是否合适？"></a>工具是否合适？</h2><p>厘清问题之后我们就可以选择合适的分析工具了。工具的何时与否还是要看我们分析的目的是什么。</p>
<h2 id="分析结果的评估？"><a href="#分析结果的评估？" class="headerlink" title="分析结果的评估？"></a>分析结果的评估？</h2><h3 id="模型的评估"><a href="#模型的评估" class="headerlink" title="模型的评估"></a>模型的评估</h3><p>模型的评估，在Data mining中对于分类和回归问题都有很多不同的评估方式（Accuracy，F1-score，SSE，MSE⋯⋯），但是具体问题要具体分析，找到合适的评估指标。用错了评估指标就会出现，像是Accuracy很高，但是模型并不fit的情况。</p>
<h3 id="结果的评估"><a href="#结果的评估" class="headerlink" title="结果的评估"></a>结果的评估</h3><p>结果的评估分为很多个方面，首先我们要看是不是有符合我们想要分析的目标。</p>
<p>如果没有得到我们预期的结果是什么原因：</p>
<ul>
<li>资料本身的属性，并不能很好的解决我们的问题（要不要多加入一些资料）</li>
<li>是不是资料的前处理有问题（像是feature的筛选、data imbalance的处理、断词、专业领域的lexicon的选择等）</li>
<li>工具的选择有些问题（有些资料可能不适合用树的模型做分类）</li>
</ul>
<h1 id="化繁为简"><a href="#化繁为简" class="headerlink" title="化繁为简"></a>化繁为简</h1><p>以前考试只觉得煎熬，想着怎么在考前最短的时间拿到还不错的成绩，然后考完就可以就瞬间把所有知识忘掉。</p>
<p>最近开始意识到其实考试是一个很好的化繁为简的过程，而且是有人帮助你将重点的知识归纳出来。</p>
<p>其实复习已经是一个强迫自己梳理知识归纳总结的过程了，但是透过考试我们能更好的找出自己梳理的重点，和老师想在这节课传达的重点之间的差距。从而补足自己这节课上的知识漏洞。意识到这一点之后，其实分数已经没那么重要了，重要的是归纳梳理的过程，以及考试之后对于自己知识漏洞的补足。</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>courses</tag>
      </tags>
  </entry>
  <entry>
    <title>豆瓣爬虫-2｜lxml 的介绍</title>
    <url>/%E8%B1%86%E7%93%A3%E7%88%AC%E8%9F%B2-2%EF%BD%9Clxml%20%E7%9A%84%E4%BB%8B%E7%B4%B9/</url>
    <content><![CDATA[<h2 id="lxml介绍"><a href="#lxml介绍" class="headerlink" title="lxml介绍"></a>lxml介绍</h2><p>Python的lxml 模组是一个非常好用且效能高的HTML、XML解析工具，通过它解析网页，爬虫就可以轻松的从网页中提取想要的资料。 lxml对XML和HTML都有很好的支援，分别使用 lxml.etree 和 lxml.html 两个模组。</p>
<h2 id="使用lxml提取网页资料的流程"><a href="#使用lxml提取网页资料的流程" class="headerlink" title="使用lxml提取网页资料的流程"></a>使用lxml提取网页资料的流程</h2><p>要从网页里面提取资料，使用lxml需要两步：</p>
<ul>
<li>第一步，用lxml把网页（或xml）解析成一个DOM树。这个过程，我们可以选择<code>etree</code>、<code>etree.HTML</code>和 <code>lxml.html</code>这三种来实现，它们基本类似但又有些许差别。</li>
<li>第二步，使用<code>xpath</code>遍历这棵DOM 树，找到你想要的资料所在的节点并提取。这一步要求我们对xpath规则比较熟练，xpath规则很多，但别怕，我来总结一些常用的套路。</li>
</ul>
<h3 id="生成DOM树"><a href="#生成DOM树" class="headerlink" title="生成DOM树"></a>生成DOM树</h3><p>上面我们说了，可以有三种方法来把网页解析成DOM树，那么我们选择哪一种呢？</p>
<p>一般HTML网页用这个比较多：<code>etree.HTML(html)</code>和<code>lxml.html(html)</code></p>
<h4 id="🌟etree-HTML-html"><a href="#🌟etree-HTML-html" class="headerlink" title="🌟etree.HTML(html)"></a>🌟etree.HTML(html)</h4><p>我们可以用这个：<code>print(etree.tostring(etree.HTML(html)).decode())</code>来将dom树还原成之前的html，我们可以发现<code>etree.HTML()</code>函式会补全html程式码片段，给它们加上<code>&lt;html&gt;</code>和<code>&lt;body&gt;</code>标签。</p>
<h4 id="🌟lxml-html-html"><a href="#🌟lxml-html-html" class="headerlink" title="🌟lxml.html(html)"></a>🌟lxml.html(html)</h4><p>lxml.html是lxml的子模组，它是对etree的封装，更适合解析html网页。生成DOM树的方法有多个：</p>
<ul>
<li>lxml.html.document_fromstring()</li>
<li>lxml.html.fragment_fromstring()</li>
<li>lxml.html.fragments_fromstring()</li>
<li>lxml.html.fromstring()</li>
</ul>
<p>我们解析网页用最后一个fromstring()即可。</p>
<h3 id="使用xpath提取资料"><a href="#使用xpath提取资料" class="headerlink" title="使用xpath提取资料"></a>使用xpath提取资料</h3><p>以下面一段简单的html为例，来介绍一下xpath对资料的提取：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"p_1 item"</span>&gt;</span>item_1<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"p_2 item"</span>&gt;</span>item_2<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"2"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">id</span>=<span class="string">"p3"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/go-p3"</span>&gt;</span>item_3<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>首先生成DOM树：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree </span><br><span class="line">doc = etree.HTML(html)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>通过标签属性定位节点:比如我们要获取<code>&lt;div class=&quot;2&quot;&gt;</code>这节点：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">doc.xpath(<span class="string">'//div[@class="2"]'</span>)</span><br><span class="line">print(etree.tostring(doc.xpath(<span class="string">'//div[@class="2"]'</span>)[<span class="number">0</span>]).decode())</span><br><span class="line"><span class="comment"># &lt;div class="2"&gt;</span></span><br><span class="line"><span class="comment">#     &lt;p id="p3"&gt;&lt;a href="/go-p3"&gt;item_3&lt;/a&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="comment"># &lt;/div&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>contains语法</li>
</ol>
<p>html中有两个<code>&lt;p&gt;</code>标签的class含有<code>item</code>，如果我们要提取这两个<code>&lt;p&gt;</code>标签，则：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 获取&lt;p&gt;的文字：</span></span><br><span class="line">doc.xpath(<span class="string">'//p[contains(@class, "item")]/text()'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt; ['item_1', 'item_2']</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>starts-with语法</li>
</ol>
<p>一样的提取需求，两个<code>&lt;p&gt;</code>标签的class都是以p_开头的，所以:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 获取&lt;p&gt;的文字：</span></span><br><span class="line">doc.xpath(<span class="string">'//p[starts-with(@class, "p_")]/text()'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt; ['item_1', 'item_2']</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>获取某一属性的值</li>
</ol>
<p>比如，我们想提取网页中所有的连结：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">doc.xpath(<span class="string">'//@href'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title>豆瓣爬虫-1｜selenium 的介紹</title>
    <url>/%E8%B1%86%E7%93%A3%E7%88%AC%E8%9F%B2-1%EF%BD%9Cselenium%20%E7%9A%84%E4%BB%8B%E7%B4%B9/</url>
    <content><![CDATA[<p>纪录一下写豆瓣爬虫的过程和一些心得</p>
<h2 id="selenium-的介绍"><a href="#selenium-的介绍" class="headerlink" title="selenium 的介绍"></a>selenium 的介绍</h2><p>selenium 是一款自动化测试利器，可以自动化模拟人的浏览器操作行为，所以也可以用于网络爬虫。<br>不过这里主要讲一讲怎样用selenium来模拟登录，并持久化cookie，然后用requests爬取页面。</p>
<h2 id="selenium-的安装及配置"><a href="#selenium-的安装及配置" class="headerlink" title="selenium 的安装及配置"></a>selenium 的安装及配置</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>直接安装 selenium </p>
<p><code>pip install selenium</code></p>
<h3 id="配置-webdriver"><a href="#配置-webdriver" class="headerlink" title="配置 webdriver"></a>配置 webdriver</h3><p>使用 selenium 前我们首先要配置浏览器的 webdriver 这边主要提供3种常用浏览器的配置页面：</p>
<ul>
<li><a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="noopener">chrome</a></li>
<li><a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">Firefox</a></li>
<li><a href="https://webkit.org/blog/6900/webdriver-support-in-safari-10/" target="_blank" rel="noopener">Safari</a></li>
</ul>
<p>⚠️ 下载前一定要查看一下自己浏览器的版本，下载适配版本的 webdriver</p>
<h3 id="配置-chromedriver（Mac版本）"><a href="#配置-chromedriver（Mac版本）" class="headerlink" title="配置 chromedriver（Mac版本）"></a>配置 chromedriver（Mac版本）</h3><p>以chromedriver为例，在下载好后，我们将 chromedriver 档案放入到：/usr/local/bin/ 这个路径下面。</p>
<p>具体的步骤：打开一个 Finder -&gt; shift+command+G -&gt; 输入路径 /usr/local/bin/ 按下return -&gt; 将 chromedriver 放入</p>
<p>⚠️ 可能会遇到的error：</p>
<p>“chromedriver” cannot be opened because the developer cannot be verified. Unable to launch the chrome browser on Mac OS</p>
<p>解决办法可以看这个<a href="https://timonweb.com/misc/fixing-error-chromedriver-cannot-be-opened-because-the-developer-cannot-be-verified-unable-to-launch-the-chrome-browser-on-mac-os/" target="_blank" rel="noopener">Blog</a>按照上面的步骤输入就可以正常运行了。</p>
<h2 id="selenium-的一些基本用法"><a href="#selenium-的一些基本用法" class="headerlink" title="selenium 的一些基本用法"></a>selenium 的一些基本用法</h2><p>控制浏览器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Chrome() <span class="comment"># 需要调用对应的chromedriver.exe</span></span><br></pre></td></tr></table></figure>

<p>使用IP代理，通常不需要都行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chrome_option = webdriver.ChromeOptions()</span><br><span class="line">chrome_option.add_argument(<span class="string">'--proxy--server=112.84.55.122:9999'</span>)<span class="comment">#使用代理IP</span></span><br></pre></td></tr></table></figure>

<p>等待网页加载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.implicitly_wait(<span class="number">5</span>) <span class="comment">#最长等待5秒，记载完成后自动跳过</span></span><br></pre></td></tr></table></figure>

<p>打开网页</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.get(url) <span class="comment"># 需要打开的网页</span></span><br></pre></td></tr></table></figure>

<p>点击网页节点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_xpath(<span class="string">'这里放网页xpath路径'</span>).click()</span><br></pre></td></tr></table></figure>

<p>输入内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_xpath(<span class="string">'这里放网页输入框的xpath路径'</span>).send_keys(<span class="string">'输入词'</span>)</span><br></pre></td></tr></table></figure>

<p>获取文本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_xpath(<span class="string">'网页中文本xpath路径'</span>).text <span class="comment">#直接获取某个具体文本</span></span><br></pre></td></tr></table></figure>

<p>网页下拉</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">js=<span class="string">"var q=document.documentElement.scrollTop=10000000"</span> <span class="comment">#滚动条，数值为每次下拉的长度，不叠加，以浏览器底部为最大值</span></span><br><span class="line">driver.execute_script(js)<span class="comment">#调用js</span></span><br></pre></td></tr></table></figure>

<h2 id="使用-selenium-模拟豆瓣登录的-demo"><a href="#使用-selenium-模拟豆瓣登录的-demo" class="headerlink" title="使用 selenium 模拟豆瓣登录的 demo"></a>使用 selenium 模拟豆瓣登录的 demo</h2><p>因为selenium是模拟人的一个登录的行为，所以首先要厘清一下我们在做登录时的一个逻辑：</p>
<ol>
<li>打开浏览器</li>
<li>打开url</li>
<li>在“短信登录”和“密码登录”中选择“密码登录”（因为短信登录会有经常报错的问题）</li>
<li>定位到“帐号”和“密码”（查看wed的html文档）</li>
<li>输入“帐号”和“密码”</li>
<li>点击“登录豆瓣”</li>
</ol>
<p>需要import的模块主要就是两个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来就是定义登录的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        登录，并持久化cookie</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 豆瓣登录页面 URL</span></span><br><span class="line">        login_url = <span class="string">'https://www.douban.com/accounts/login'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取chrome的配置</span></span><br><span class="line">        opt = webdriver.ChromeOptions()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在运行的时候不弹出浏览器窗口</span></span><br><span class="line">        <span class="keyword">if</span> self.headless:</span><br><span class="line">            opt.set_headless()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取driver对象</span></span><br><span class="line">        self.driver = webdriver.Chrome(chrome_options = opt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 打开登录页面</span></span><br><span class="line">        self.driver.get(login_url)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'[login] opened login page...'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 向浏览器发送用户名、密码，并点击登录按钮</span></span><br><span class="line">        <span class="comment"># 首先点击“密码登录”，从手机号登陆转到密码登录（因为手机号会区域登陆异常的问题）</span></span><br><span class="line">        self.driver.find_element_by_class_name(<span class="string">'account-tab-account'</span>).click()</span><br><span class="line">        <span class="comment"># 用 clear()清空一下之前的用户名和密码，然后用 send_keys()发送一下用户名和密码</span></span><br><span class="line">        self.driver.find_element_by_name(<span class="string">'username'</span>).clear()</span><br><span class="line">        self.driver.find_element_by_name(<span class="string">'username'</span>).send_keys(self.user_name)</span><br><span class="line">        self.driver.find_element_by_name(<span class="string">'password'</span>).clear()</span><br><span class="line">        self.driver.find_element_by_name(<span class="string">'password'</span>).send_keys(self.password)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 多次登录需要输入验证码，这里给一个手工输入验证码的时间</span></span><br><span class="line">        <span class="comment"># 等待 3 秒钟</span></span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定位到“登录按钮”超链接信息上面的文本元素</span></span><br><span class="line">        self.driver.find_element_by_link_text(<span class="string">'登录豆瓣'</span>).click()</span><br><span class="line">        print(<span class="string">'[login] submited...'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 等待 3 秒钟</span></span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 推出 自动关闭</span></span><br><span class="line">        driver.quit()</span><br></pre></td></tr></table></figure>

<p>👆就完成了一个简单的用户登入的行为</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title>文字探勘-1｜资料伦理与分析陷阱</title>
    <url>/%E6%96%87%E5%AD%97%E6%8E%A2%E5%8B%98-1%EF%BD%9C%E8%B3%87%E6%96%99%E5%80%AB%E7%90%86%E8%88%87%E5%88%86%E6%9E%90%E9%99%B7%E9%98%B1/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>“社媒”这一系列主要用以梳理“社群媒体分析”这堂课的知识点以及纪录一些课后的思考🤔。</p>
<h2 id="资料伦理"><a href="#资料伦理" class="headerlink" title="资料伦理"></a>资料伦理</h2><h2 id="🌰-Case-Automated-Health-Care-App"><a href="#🌰-Case-Automated-Health-Care-App" class="headerlink" title="🌰 Case: Automated Health Care App"></a>🌰 Case: Automated Health Care App</h2><p>来源：<a href="https://aiethics.princeton.edu/case-studies/case-study-pdfs/" target="_blank" rel="noopener">https://aiethics.princeton.edu/case-studies/case-study-pdfs/</a></p>
<p>简介：Charlie 是一款智能健康管理的app</p>
<ul>
<li>利用只会手表来监测患者的血糖</li>
<li>利用AI演算法，来计算药剂的注射量，以及生活型态的建议</li>
<li>并设置论坛（furom）来供病友互相交流，并且提供最新的资讯。</li>
</ul>
<h3 id="旧版："><a href="#旧版：" class="headerlink" title="旧版："></a>旧版：</h3><p>⚠️ 面临的问题：</p>
<ul>
<li>虽然大部分使用者对降低血糖都有作用，但是少数族裔，和少部分一些人并没有改善</li>
<li>在论坛中出现了同温层现象，并互相攻击</li>
<li>有一些人离开了</li>
</ul>
<p>上述👆问题的分析：</p>
<ul>
<li>问题1：少数族裔没有效果很可能是，他们的资料量太小，没有足够的资料来建立模型，来给这一部分人群提供比较完善的医疗方案。<ul>
<li>我的一些小困惑 😖<ul>
<li>差异化的模型？ ？</li>
<li>所以这种模型要怎么训练？ ？</li>
<li>是不是只有应用之后才能知道不适合所有人，从而做出差异化的模型？ ？</li>
<li>前期的数据探索是否能看出一些端倪？ ？</li>
<li>所以是不是只要数据够多，模型就可以cover这种差异化？ ？ ）</li>
</ul>
</li>
</ul>
</li>
<li>问题2：人喜欢看符合自己价值观的</li>
</ul>
<p>解决方案：</p>
<ul>
<li>问题1：<ul>
<li>收集更多少数族裔的资料</li>
</ul>
</li>
<li>问题2：<ul>
<li>减少病人接触非主流的报导</li>
<li>同温层：不按时间次序显示文章和留言；被赞较多的优先</li>
</ul>
</li>
<li>问题3：<ul>
<li>个性化推荐文章，使用（MAB）</li>
</ul>
</li>
</ul>
<h3 id="新版："><a href="#新版：" class="headerlink" title="新版："></a>新版：</h3><p>运行结果：基本解决上述的3个问题</p>
<p>⚠️ 但是！一些人产生了担忧：</p>
<ul>
<li>文章遭到公司的过滤，替病友决定了什么是该看的文章（家长式领导Paternalism）</li>
<li>MAB的推荐系统的文章可能对病友造成伤害</li>
<li>Charlie演算法运作机制不明，缺乏透明度</li>
</ul>
<p>团队的反馈：</p>
<ul>
<li>文章的过滤基于的是医学专业的考量</li>
<li>MAB推荐的文章都是经过认证的</li>
<li>AI虽有黑箱的问题，但是人类专家得出结论，一般病人也不会询问医生是如何判断的。</li>
</ul>
<h2 id="资料分析的陷阱"><a href="#资料分析的陷阱" class="headerlink" title="资料分析的陷阱"></a>资料分析的陷阱</h2><p>资料分析常常被拿来误导：</p>
<ul>
<li>误导可以带来利益</li>
<li>误导的信息传播的特别快</li>
<li>人类的同理心可以产生完美的误导</li>
</ul>
<p>误导产生的面向：</p>
<ul>
<li>人本身：<ul>
<li>人本身跟偏爱和自己立场一致的文章：很可能并不会看完文章的，判断对错，只是因为立场一致而就进一步传播</li>
<li>同温层的产生</li>
</ul>
</li>
<li>机器的诱导：<ul>
<li>演算法为增加用户黏性而一直只推荐用户喜欢的文章：使用户只能看到和自己立场相似的文章，而看不到客观的事实。</li>
</ul>
</li>
<li>结果的呈现：<ul>
<li>修改图表的scale从而达到表达自己想要的结论的目的。</li>
<li>相关性常被误导为因果关系</li>
<li>观察的偏差：观察角度的不同可能会有完全不一样的结论（如何避免呢？？？）</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>courses</tag>
      </tags>
  </entry>
  <entry>
    <title>文字探勘-0｜基本流程</title>
    <url>/%E6%96%87%E5%AD%97%E6%8E%A2%E5%8B%98-0%EF%BD%9C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="文字挖掘的基本流程："><a href="#文字挖掘的基本流程：" class="headerlink" title="文字挖掘的基本流程："></a>文字挖掘的基本流程：</h3><p><img src="https://img-blog.csdnimg.cn/20200305221146509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="基本处理步骤："><a href="#基本处理步骤：" class="headerlink" title="基本处理步骤："></a>基本处理步骤：</h3><h4 id="1-抓取非结构化的资料："><a href="#1-抓取非结构化的资料：" class="headerlink" title="1. 抓取非结构化的资料："></a>1. 抓取非结构化的资料：</h4><ul>
<li><p>定义资料的来源：<br> 新闻、社群网络、论坛、文献资料（BBC、微博、豆瓣、知乎、各大学术期刊）</p>
</li>
<li><p>定义关键字找出相关文章：<br> 白名单：与想要分析的资料相关的一些关键字<br> 黑名单：与白名单相似的，但与要分析的资料无关的干扰项</p>
</li>
<li><p>过滤假的信息：<br> 有时我们需要分析留言，评论的内容，而这其中常常参杂着很多（有文章指出大约在16%，有些可能会更多）造假的评论和留言。这些是我们希望尽可能的去避免，去过滤掉的。</p>
<ul>
<li>依内容</li>
<li>依发布时间（像是一般一个产品发布初期可能会有水军来刷评价）</li>
<li>依作者</li>
</ul>
</li>
</ul>
<h4 id="2-资料的前处理"><a href="#2-资料的前处理" class="headerlink" title="2. 资料的前处理"></a>2. 资料的前处理</h4><p>目的：转成正规的语句</p>
<ul>
<li>格式统一<ul>
<li>去除格式话的标签</li>
<li>半形全形的转换</li>
</ul>
</li>
<li>去除或取代一些数字和符号<ul>
<li>ε=ε=ε=ε=ε=ε=┌(;￣◇￣)┘：逃跑</li>
<li>awsl “啊我死了”：太可爱<ul>
<li>更正一些标点符号</li>
</ul>
</li>
<li>有些帖文没有标点</li>
<li>中文英文对于语句结束的定义不同</li>
<li>去除引号和一些无法处理的符号</li>
</ul>
</li>
</ul>
<h4 id="3-断句断词"><a href="#3-断句断词" class="headerlink" title="3. 断句断词"></a>3. 断句断词</h4><p>断句的目的：我们可以看一下一篇文章有几个句子，每个句子的长度。通过分析每个句子的长度和复杂度我们可以侧面看出教育水平。</p>
<p>断词的目的：中文都是以字为单位，但是要分析的话我们需要以词为单位来分析才有意义。</p>
<p><strong>注：</strong> 我们常常需要根据不同的分析案例来自建词典。（jiebaR提供自己自建词典）因为不同领域的一些专有名词在常用的词典中没有。</p>
<h4 id="4-去除停用字"><a href="#4-去除停用字" class="headerlink" title="4. 去除停用字"></a>4. 去除停用字</h4><p>断词后去除一些缀词，口头语，感叹词（像是：啊，哦，你，我，他，总之）</p>
<h4 id="5-根据所需进行分析和应用"><a href="#5-根据所需进行分析和应用" class="headerlink" title="5. 根据所需进行分析和应用"></a>5. 根据所需进行分析和应用</h4><ul>
<li>文字云</li>
<li>知识图谱</li>
<li>对话机器人</li>
<li>……</li>
</ul>
<h3 id="需要的工具套件："><a href="#需要的工具套件：" class="headerlink" title="需要的工具套件："></a>需要的工具套件：</h3><ol>
<li>内容来源：guternbergr 古腾堡计划，提供很多书籍电子版资料，可以用guternbergr套件以编号直接下载书籍</li>
<li>自己爬取内容：现成的爬虫工具，自己写爬虫</li>
<li>基本文字处理：dplyr、tidytext 包</li>
<li>断词模组：<pre><code>1. 中文：jiebar包
    基本断词
    自定义使用者断词词典
    自定义停用词
 2. 英文：Stanford Cote NLP</code></pre></li>
<li>词性标注模组：<br>  中文：Stanford Cote NLP<br>  英文：Stanford Cote NLP</li>
</ol>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB 的载入</title>
    <url>/MongoDB%20%E7%9A%84%E8%BC%89%E5%85%A5/</url>
    <content><![CDATA[<h2 id="安装-MongoDB"><a href="#安装-MongoDB" class="headerlink" title="安装 MongoDB"></a>安装 MongoDB</h2><p>安裝 Mongo3.2</p>
<p>Windows：<a href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/</a></p>
<p>Mac OS：<a href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/</a></p>
<p>安裝 Studio 3T（MongoDB的可视化工具）</p>
<p><a href="https://robomongo.org/" target="_blank" rel="noopener">https://robomongo.org/</a></p>
<h2 id="将资料汇入-Studio-3T"><a href="#将资料汇入-Studio-3T" class="headerlink" title="将资料汇入 Studio 3T"></a>将资料汇入 Studio 3T</h2><h3 id="Create-database"><a href="#Create-database" class="headerlink" title="Create database"></a>Create database</h3><p><img src="https://github.com/skyexx/skyexx.github.io/blob/master/image/MongoDB/MongoDB_1.png?raw=true" alt="GitHub"> </p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>mongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>研究生申请自传</title>
    <url>/%E9%80%89%E4%B8%93%E4%B8%9A/</url>
    <content><![CDATA[<blockquote>
<p>研究生的申请自传，对自己过去学习经历的复盘和反思<br>大一大二时，因为专业的原因，我们需要学习数学、物理、化学、生物以及地科相关的知识。学习这些庞杂科目的过程，帮助我逐渐建立起了基本的科学素养。透过不断的练习「看懂」，然后「分析」与「归纳」知识点，培养了我对于科研的基本鉴赏力。不过这个阶段的我还是停留在被动的吸收和获取的老师所讲的知识。而真正改变我，并使我萌生了转换专业想法的是在大三大四修的三门课程。<br>一门是古海洋学，这节课的上课形式是，自行阅读相关主题文献，上课时间全部以同学发问，老师来回答的方式进行。所以为了能提出逻辑自洽，有科学意涵的问题，我开始认真研究每篇论文的研究逻辑，试着从作者的角度出发多去思考。这种提问的学习方式真的让我受益匪浅，不但在后续的学习中帮助我迅速的进入一些较为陌生的领域，还也让我从最初只会被动吸收老师所讲的知识，转变为自己主动思考，主动探索自己未知领域，主动获取知识。<br>另一门课则是程式设计。这节课是海下科技研究所开设的，主要是用matlab 和python 来编写程式。期中的作业是编写一个特定领域的小型搜索引擎。而期末则是编写一个可以控制十字滑轨写字画画的程序。在编写程式、设计程式架构，甚至是设计一些小彩蛋时，都让我充分体会了创造的乐趣。这也让我发现相比较与海科系注重发现和解释现象的能力，我更喜欢创造新鲜事物带来的乐趣。<br>最后一门就是资讯管理系的巨量资料分析，在这门课中我第一次接触到了机器学习，R语言以及各种算法，所以对我来说是一个不小的挑战。从最开始需要学习大量新知识时的手足无措，到后面慢慢了解数据处理的每一步逻辑，再到为了能更好的理解模型建立过程，我参加的台新银行的金融大数据分析比赛，虽然没有取得很好的名次，但是在过程中确实让我迅速学习到了各种模型以及调参的知识，也我看到机器学习在各领域的潜力，同时激起了我深入学习的热情。这门课的学习也使我坚定了转换跑道的想法。<br>海科系和资管系看似相去甚远，但其实，很多能力是想通的。四年的海科训练，教会了我如何提问，如何思考和严谨的解释一件事情。学会提问，能够帮助我在进入一个新领域时，迅速厘清这一领域的整体框架，并捕捉到自己的盲点。而严谨的思考和解释，帮助我在考虑问题的时候从多个角度出发，更好的顾及到方方面面。</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>pandas 数据分析实例</title>
    <url>/pandas%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E4%BE%8B/</url>
    <content><![CDATA[<p>话不多说就开始吧！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读入 csv 文字档</span></span><br><span class="line">gapminder = pd.read_csv(<span class="string">'gapminder.csv'</span>)</span><br><span class="line"><span class="comment"># 读取excel档 gapminder = pd.read_excel(xlsx_file)</span></span><br><span class="line">print(type(gapminder))</span><br><span class="line">gapminder.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>continent</th>
      <th>year</th>
      <th>lifeExp</th>
      <th>pop</th>
      <th>gdpPercap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1952</td>
      <td>28.801</td>
      <td>8425333</td>
      <td>779.445314</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1957</td>
      <td>30.332</td>
      <td>9240934</td>
      <td>820.853030</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1962</td>
      <td>31.997</td>
      <td>10267083</td>
      <td>853.100710</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1967</td>
      <td>34.020</td>
      <td>11537966</td>
      <td>836.197138</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1972</td>
      <td>36.088</td>
      <td>13079460</td>
      <td>739.981106</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gapminder.shape <span class="comment"># 查看 DataFrame有几行几列</span></span><br></pre></td></tr></table></figure>

<pre><code>(1704, 6)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gapminder.columns <span class="comment"># 查看 DataFrame 的变数函数</span></span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;country&apos;, &apos;continent&apos;, &apos;year&apos;, &apos;lifeExp&apos;, &apos;pop&apos;, &apos;gdpPercap&apos;], dtype=&apos;object&apos;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gapminder.index <span class="comment"># 查看 DataFrame 的列索引信息</span></span><br></pre></td></tr></table></figure>

<pre><code>RangeIndex(start=0, stop=1704, step=1)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gapminder.info() <span class="comment"># 查看 DataFrame 的详细信息</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 1704 entries, 0 to 1703
Data columns (total 6 columns):
country      1704 non-null object
continent    1704 non-null object
year         1704 non-null int64
lifeExp      1704 non-null float64
pop          1704 non-null int64
gdpPercap    1704 non-null float64
dtypes: float64(2), int64(2), object(2)
memory usage: 80.0+ KB</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gapminder.describe() <span class="comment"># 查看 DataFrame 各变量的描述统计</span></span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>lifeExp</th>
      <th>pop</th>
      <th>gdpPercap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>1704.00000</td>
      <td>1704.000000</td>
      <td>1.704000e+03</td>
      <td>1704.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>1979.50000</td>
      <td>59.474439</td>
      <td>2.960121e+07</td>
      <td>7215.327081</td>
    </tr>
    <tr>
      <td>std</td>
      <td>17.26533</td>
      <td>12.917107</td>
      <td>1.061579e+08</td>
      <td>9857.454543</td>
    </tr>
    <tr>
      <td>min</td>
      <td>1952.00000</td>
      <td>23.599000</td>
      <td>6.001100e+04</td>
      <td>241.165877</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>1965.75000</td>
      <td>48.198000</td>
      <td>2.793664e+06</td>
      <td>1202.060309</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>1979.50000</td>
      <td>60.712500</td>
      <td>7.023596e+06</td>
      <td>3531.846989</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>1993.25000</td>
      <td>70.845500</td>
      <td>1.958522e+07</td>
      <td>9325.462346</td>
    </tr>
    <tr>
      <td>max</td>
      <td>2007.00000</td>
      <td>82.603000</td>
      <td>1.318683e+09</td>
      <td>113523.132900</td>
    </tr>
  </tbody>
</table>
</div>



<p>数据整理</p>
<p>dplyr的基本功能是六个能与SQL查询语法相互呼应的函数：</p>
<p>filter（）函数：SQL查询中的where描述</p>
<p>select（）函数：SQL查询中的select描述</p>
<p>mutate（）函数：SQL查询中的衍生字段描述</p>
<p>arrange（）函数：SQL查询中的order by描述</p>
<p>summarise（）函数：SQL查询中的聚合函数描述</p>
<p>group_by（）函数：SQL查询中的group by描述</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 撰写布尔判断条件将符合条件的观测值从数据框中筛选出，</span></span><br><span class="line"><span class="comment"># 实践filter（）函数的功能，例如选出China：</span></span><br><span class="line">gapminder[gapminder[<span class="string">'country'</span>] == <span class="string">'China'</span>]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>continent</th>
      <th>year</th>
      <th>lifeExp</th>
      <th>pop</th>
      <th>gdpPercap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>288</td>
      <td>China</td>
      <td>Asia</td>
      <td>1952</td>
      <td>44.00000</td>
      <td>556263527</td>
      <td>400.448611</td>
    </tr>
    <tr>
      <td>289</td>
      <td>China</td>
      <td>Asia</td>
      <td>1957</td>
      <td>50.54896</td>
      <td>637408000</td>
      <td>575.987001</td>
    </tr>
    <tr>
      <td>290</td>
      <td>China</td>
      <td>Asia</td>
      <td>1962</td>
      <td>44.50136</td>
      <td>665770000</td>
      <td>487.674018</td>
    </tr>
    <tr>
      <td>291</td>
      <td>China</td>
      <td>Asia</td>
      <td>1967</td>
      <td>58.38112</td>
      <td>754550000</td>
      <td>612.705693</td>
    </tr>
    <tr>
      <td>292</td>
      <td>China</td>
      <td>Asia</td>
      <td>1972</td>
      <td>63.11888</td>
      <td>862030000</td>
      <td>676.900092</td>
    </tr>
    <tr>
      <td>293</td>
      <td>China</td>
      <td>Asia</td>
      <td>1977</td>
      <td>63.96736</td>
      <td>943455000</td>
      <td>741.237470</td>
    </tr>
    <tr>
      <td>294</td>
      <td>China</td>
      <td>Asia</td>
      <td>1982</td>
      <td>65.52500</td>
      <td>1000281000</td>
      <td>962.421380</td>
    </tr>
    <tr>
      <td>295</td>
      <td>China</td>
      <td>Asia</td>
      <td>1987</td>
      <td>67.27400</td>
      <td>1084035000</td>
      <td>1378.904018</td>
    </tr>
    <tr>
      <td>296</td>
      <td>China</td>
      <td>Asia</td>
      <td>1992</td>
      <td>68.69000</td>
      <td>1164970000</td>
      <td>1655.784158</td>
    </tr>
    <tr>
      <td>297</td>
      <td>China</td>
      <td>Asia</td>
      <td>1997</td>
      <td>70.42600</td>
      <td>1230075000</td>
      <td>2289.234136</td>
    </tr>
    <tr>
      <td>298</td>
      <td>China</td>
      <td>Asia</td>
      <td>2002</td>
      <td>72.02800</td>
      <td>1280400000</td>
      <td>3119.280896</td>
    </tr>
    <tr>
      <td>299</td>
      <td>China</td>
      <td>Asia</td>
      <td>2007</td>
      <td>72.96100</td>
      <td>1318683096</td>
      <td>4959.114854</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果有多个条件，可以使用|或&amp;amp;符号连接，例如选出2007年的亚洲国家(用.iloc选择显示前几行)：</span></span><br><span class="line">gapminder[(gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>) &amp; (gapminder[<span class="string">'continent'</span>] == <span class="string">'Asia'</span>)].iloc[<span class="number">0</span>:<span class="number">10</span>,]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>continent</th>
      <th>year</th>
      <th>lifeExp</th>
      <th>pop</th>
      <th>gdpPercap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>11</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>2007</td>
      <td>43.828</td>
      <td>31889923</td>
      <td>974.580338</td>
    </tr>
    <tr>
      <td>95</td>
      <td>Bahrain</td>
      <td>Asia</td>
      <td>2007</td>
      <td>75.635</td>
      <td>708573</td>
      <td>29796.048340</td>
    </tr>
    <tr>
      <td>107</td>
      <td>Bangladesh</td>
      <td>Asia</td>
      <td>2007</td>
      <td>64.062</td>
      <td>150448339</td>
      <td>1391.253792</td>
    </tr>
    <tr>
      <td>227</td>
      <td>Cambodia</td>
      <td>Asia</td>
      <td>2007</td>
      <td>59.723</td>
      <td>14131858</td>
      <td>1713.778686</td>
    </tr>
    <tr>
      <td>299</td>
      <td>China</td>
      <td>Asia</td>
      <td>2007</td>
      <td>72.961</td>
      <td>1318683096</td>
      <td>4959.114854</td>
    </tr>
    <tr>
      <td>671</td>
      <td>Hong Kong, China</td>
      <td>Asia</td>
      <td>2007</td>
      <td>82.208</td>
      <td>6980412</td>
      <td>39724.978670</td>
    </tr>
    <tr>
      <td>707</td>
      <td>India</td>
      <td>Asia</td>
      <td>2007</td>
      <td>64.698</td>
      <td>1110396331</td>
      <td>2452.210407</td>
    </tr>
    <tr>
      <td>719</td>
      <td>Indonesia</td>
      <td>Asia</td>
      <td>2007</td>
      <td>70.650</td>
      <td>223547000</td>
      <td>3540.651564</td>
    </tr>
    <tr>
      <td>731</td>
      <td>Iran</td>
      <td>Asia</td>
      <td>2007</td>
      <td>70.964</td>
      <td>69453570</td>
      <td>11605.714490</td>
    </tr>
    <tr>
      <td>743</td>
      <td>Iraq</td>
      <td>Asia</td>
      <td>2007</td>
      <td>59.545</td>
      <td>27499638</td>
      <td>4471.061906</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用list标注变数名称，可以将所需变数的一列提取出来</span></span><br><span class="line">gapminder[[<span class="string">'country'</span>, <span class="string">'continent'</span>]].iloc[<span class="number">0</span>:<span class="number">10</span>,]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>continent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>8</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
    <tr>
      <td>9</td>
      <td>Afghanistan</td>
      <td>Asia</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 直接撰写衍生公式并为变数命名即可实践mutate（）函数的功能，搭配apply（）与lambda函数将公式应用到每一个观测值，</span></span><br><span class="line"><span class="comment"># 例如新增一个country_abb变数撷取原本country变数的前三个英文字母：</span></span><br><span class="line">gapminder[<span class="string">'country_abb'</span>] = gapminder[<span class="string">'country'</span>].apply(<span class="keyword">lambda</span> x: x[:<span class="number">3</span>])</span><br><span class="line">gapminder.iloc[<span class="number">1</span>:<span class="number">10</span>,]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>continent</th>
      <th>year</th>
      <th>lifeExp</th>
      <th>pop</th>
      <th>gdpPercap</th>
      <th>country_abb</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1957</td>
      <td>30.332</td>
      <td>9240934</td>
      <td>820.853030</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1962</td>
      <td>31.997</td>
      <td>10267083</td>
      <td>853.100710</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1967</td>
      <td>34.020</td>
      <td>11537966</td>
      <td>836.197138</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1972</td>
      <td>36.088</td>
      <td>13079460</td>
      <td>739.981106</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1977</td>
      <td>38.438</td>
      <td>14880372</td>
      <td>786.113360</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1982</td>
      <td>39.854</td>
      <td>12881816</td>
      <td>978.011439</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1987</td>
      <td>40.822</td>
      <td>13867957</td>
      <td>852.395945</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>8</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1992</td>
      <td>41.674</td>
      <td>16317921</td>
      <td>649.341395</td>
      <td>Afg</td>
    </tr>
    <tr>
      <td>9</td>
      <td>Afghanistan</td>
      <td>Asia</td>
      <td>1997</td>
      <td>41.763</td>
      <td>22227415</td>
      <td>635.341351</td>
      <td>Afg</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 呼叫DataFrame不同的聚合函数针对字段计算，实践summarise（）函数的功能，例如计算2007年全球人口总数：</span></span><br><span class="line">gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>][[<span class="string">'pop'</span>]].sum()</span><br></pre></td></tr></table></figure>

<pre><code>pop    6251013179
dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者计算 2007 年全球的平均寿命、平均财富：</span></span><br><span class="line">gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>][[<span class="string">'lifeExp'</span>, <span class="string">'gdpPercap'</span>]].mean()</span><br></pre></td></tr></table></figure>

<pre><code>lifeExp         67.007423
gdpPercap    11680.071820
dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 最后用 DataFrame的 groupby 方法实践 group_by（）函数的功能，例如计算2007年各洲人口总数：</span></span><br><span class="line">gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>].groupby(by = <span class="string">'continent'</span>)[<span class="string">'pop'</span>].sum()</span><br></pre></td></tr></table></figure>

<pre><code>continent
Africa       929539692
Americas     898871184
Asia        3811953827
Europe       586098529
Oceania       24549947
Name: pop, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者计算2007年各洲平均寿命、平均财富：</span></span><br><span class="line">gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>].groupby(by = <span class="string">'continent'</span>)[<span class="string">'lifeExp'</span>, <span class="string">'gdpPercap'</span>].mean()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>lifeExp</th>
      <th>gdpPercap</th>
    </tr>
    <tr>
      <th>continent</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Africa</td>
      <td>54.806038</td>
      <td>3089.032605</td>
    </tr>
    <tr>
      <td>Americas</td>
      <td>73.608120</td>
      <td>11003.031625</td>
    </tr>
    <tr>
      <td>Asia</td>
      <td>70.728485</td>
      <td>12473.026870</td>
    </tr>
    <tr>
      <td>Europe</td>
      <td>77.648600</td>
      <td>25054.481636</td>
    </tr>
    <tr>
      <td>Oceania</td>
      <td>80.719500</td>
      <td>29810.188275</td>
    </tr>
  </tbody>
</table>
</div>



<p>Python可视化的基石是Matplotlib套件的pyplot，她的绘图哲学是将图形的元素，例如坐标轴、线、点或者文字用不同的方法一一拼凑起来，优点是绘图的弹性非常高，缺点则是对于初学者的门坎略高。为了解决这个问题，pandas套件将matplotlib.pyplot的基础图形包装起来成为一个方法，让使用者只要呼叫df.plot（）就能够便利地绘图，可以选择的图形种类相当丰富，只要指定kind =参数即可：<br>line’：线图（预设）<br>‘bar’：垂直直方图<br>‘barh’：水平直方图<br>‘hist’：直方图<br>‘box’：盒须图<br>‘scatter’：散布图<br>‘hexbin’：hexbin plot<br>…etc.<br>在作图之前我们加载matplotlib.pyplot与seaborn，前者是绘图的基础套件，后者是让图形的样式美观：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化时间与数值：线图</span></span><br><span class="line"><span class="comment"># 将中国数据筛选出来并绘制从1952年至2007年的人口变化：</span></span><br><span class="line"></span><br><span class="line">gapminder_cn = gapminder[gapminder[<span class="string">'country'</span>] == <span class="string">'China'</span>]</span><br><span class="line">gapminder_cn[[<span class="string">'year'</span>, <span class="string">'pop'</span>]].plot(kind = <span class="string">'line'</span>, x = <span class="string">'year'</span>, y = <span class="string">'pop'</span>, title = <span class="string">'Pop vs. Year in China'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212022335.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者将中国、日本、韩国数据筛选出来并绘制从1952年至2007年的平均寿命变化</span></span><br><span class="line"></span><br><span class="line">gapminder_northasia = gapminder.loc[gapminder[<span class="string">'country'</span>].isin([<span class="string">'China'</span>, <span class="string">'Japan'</span>, <span class="string">'Korea, Rep.'</span>])]</span><br><span class="line">gapminder_northasia_pivot = gapminder_northasia.pivot_table(values = <span class="string">'lifeExp'</span>, columns = <span class="string">'country'</span>, index = <span class="string">'year'</span>)</span><br><span class="line">gapminder_northasia_pivot.plot(title = <span class="string">'Life Expectancies in North Asia'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212035991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化数值的分布：直方图、盒须图</span></span><br><span class="line"><span class="comment"># 将2007年数据筛选出来并以三个子图（subplots）绘制人口数、平均寿命与人均所得的直方图：</span></span><br><span class="line"></span><br><span class="line">gapminder_2007 = gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>]</span><br><span class="line">gapminder_2007[[<span class="string">'pop'</span>, <span class="string">'gdpPercap'</span>, <span class="string">'lifeExp'</span>]].hist(bins = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212057731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者绘制人均所得的直方图：</span></span><br><span class="line">gapminder_2007[[<span class="string">'gdpPercap'</span>]].plot(kind = <span class="string">'hist'</span>, title = <span class="string">'GDP Per Capita in 2007'</span>, legend = <span class="literal">False</span>, bins = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212111699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者将人均所得直方图依照不同洲别以不同颜色绘制：</span></span><br><span class="line">gapminder_continent_pivot = gapminder_2007.pivot_table(values = <span class="string">'gdpPercap'</span>, columns = <span class="string">'continent'</span>, index = <span class="string">'country'</span>)</span><br><span class="line">gapminder_continent_pivot.plot(kind = <span class="string">'hist'</span>, alpha=<span class="number">0.5</span>, bins = <span class="number">20</span>, title = <span class="string">'GDP Per Capita by Continent'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212122981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者依照不同洲别，将人均所得以盒须图绘制</span></span><br><span class="line">gapminder_continent_pivot.plot(kind = <span class="string">'box'</span>, title = <span class="string">'GDP Per Capita by Continent'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212137412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化相关性：散点图、hexbin plot</span></span><br><span class="line">gapminder_2007.plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'gdpPercap'</span>, y = <span class="string">'lifeExp'</span>, title = <span class="string">'Wealth vs. Health in 2007'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 改为hexbin plot</span></span><br><span class="line">gapminder_2007.plot(kind = <span class="string">'hexbin'</span>, x = <span class="string">'gdpPercap'</span>, y = <span class="string">'lifeExp'</span>, title = <span class="string">'Wealth vs. Health in 2007'</span>, gridsize = <span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401212149915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/2020040121221111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化排名：直方图</span></span><br><span class="line"><span class="comment"># 绘制2007年各洲的人口总数：</span></span><br><span class="line">summarized_df = gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>].groupby(by = <span class="string">'continent'</span>)[<span class="string">'pop'</span>].sum()</span><br><span class="line">summarized_df.plot(kind = <span class="string">'bar'</span>, rot = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/2020040121222379.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 或者绘制2007年各洲平均寿命、平均财富：</span></span><br><span class="line">mean_df = gapminder[gapminder[<span class="string">'year'</span>] == <span class="number">2007</span>].groupby(<span class="string">'continent'</span>)[<span class="string">'lifeExp'</span>,<span class="string">'gdpPercap'</span>].mean()</span><br><span class="line">mean_df = mean_df.reset_index()</span><br><span class="line"></span><br><span class="line">mean_df.head()</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot(mean_df[<span class="string">'continent'</span>], mean_df[<span class="string">'lifeExp'</span>], <span class="string">'-'</span>, label = <span class="string">'lifeExp'</span>)</span><br><span class="line">ax2 = ax.twinx()</span><br><span class="line">ax2.plot(mean_df[<span class="string">'continent'</span>], mean_df[<span class="string">'gdpPercap'</span>], <span class="string">'-r'</span>, label = <span class="string">'gdpPercap'</span>)</span><br><span class="line">ax.set_ylim(<span class="number">40</span>,<span class="number">100</span>)</span><br><span class="line">ax2.set_ylim(<span class="number">0</span>, <span class="number">50000</span>)</span><br><span class="line">ax.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">ax2.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200401223604799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
      </tags>
  </entry>
  <entry>
    <title>Github 图床测试</title>
    <url>/article/</url>
    <content><![CDATA[<h2 id="this-is-test"><a href="#this-is-test" class="headerlink" title="this is test"></a>this is test</h2><p><img src="https://img-blog.csdnimg.cn/20200311110734967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="file"></p>
]]></content>
      <categories>
        <category>Commercial</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>模型的评估：性能量度</title>
    <url>/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%EF%BC%9A%E6%80%A7%E8%83%BD%E9%87%8F%E5%BA%A6/</url>
    <content><![CDATA[<p>对学习器的泛化能力的评估需要一套标准，也就是性能量度（performance measure）。使用不同的性能量度往往会导致不同的评判结果，所以<strong>模型的好坏是相对的</strong>。所以什么样的模型是好的不仅取决于算法和数据，还取决于任务需求。不要以为的掉进更复杂更难的模型就一定更好的陷阱。</p>
<p>回到模型评估</p>
<h2 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h2><p>回归任务中，我们要预测一个连续的值，最常见的就是“均方误差”（MSE）<br><img src="https://img-blog.csdnimg.cn/20200318183918369.png" alt="在这里插入图片描述"></p>
<h2 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h2><h3 id="1、正确率、错误率、-查准率、查全率"><a href="#1、正确率、错误率、-查准率、查全率" class="headerlink" title="1、正确率、错误率、 查准率、查全率"></a>1、正确率、错误率、 查准率、查全率</h3><p>分类任务中，我们需要预测的是离散值，主要用到的几种数据：</p>
<p><strong>True Positives(TP)</strong>：实际为正例且被分类器分为正例的个数<br><strong>False Positives(FP)</strong>：实际为负例且被分类器分为正例的个数<br><strong>False Negatives(FN)</strong>：实际为正例且被分类器分为负例的个数<br><strong>True Negatives(TN)</strong>：实际为负例且被分类器分为负例的个数<br><strong>TP + FN = P</strong>：实际的正例个数<br><strong>FP + TN = N</strong>：实际的负例个数<br><img src="https://img-blog.csdnimg.cn/20200318195314102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>正确率（accuracy）</strong>:</p>
<ul>
<li><strong>accuracy = (TP + TN)/(P + N)</strong><br>看上去有点绕，其实非常简单就是：==分类正确的个数 / 总的样本数==</li>
</ul>
<p><strong>错误率（error rate）</strong>: </p>
<ul>
<li><strong>error_rate = (FP + FN)/(P + N) = ==1 - accuracy==</strong><br>分类错误的个数除以总的样本数</li>
</ul>
<p><strong>查准率/精度（precision）</strong>:</p>
<ul>
<li><strong>precision = TP/(TP + FP)</strong><br>正确分为正例的个数除以正确分为正例和错误分为正例的个数之和<br>==预测出正例的有多少正确的==</li>
</ul>
<p><strong>查全率/召回率（recall）</strong>: </p>
<ul>
<li><strong>recall = TP/(TP + FN) = TP/P</strong><br>正确分为正例的个数除以正例的总数，也称之为灵敏度(sensitive)<br>==预测正确的正例占总正例的比例==</li>
</ul>
<p><strong>一般情况下，在一定正确率前提下，要求分类器的召回率尽量高。</strong> </p>
<p><img src="https://img-blog.csdnimg.cn/20200318190948445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="查准率和查全率"></p>
<h3 id="2、P-R曲线"><a href="#2、P-R曲线" class="headerlink" title="2、P-R曲线"></a>2、P-R曲线</h3><p>P-R曲线就是：查准率（precision）/ 查全率（recall）的曲线</p>
<p>根据模型的预测结果（一般可能会是概率）进行排序，将最可能的正样本排在前面，将最不可能的正样本排在后面，按此顺序逐个设定阈值，大于阈值的为正样本，每次计算出当前的P值和R值，如图 。<br><img src="https://img-blog.csdnimg.cn/20200318192033543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>多个模型的P-R曲线的评估遵循：若B包裹住了C，则B优于C。若两条线出现交叉，则根据平衡点（P=R）判断，越高，性能越好。</p>
<h3 id="3、F1"><a href="#3、F1" class="headerlink" title="3、F1"></a>3、F1</h3><p>P-R值有时会出现矛盾的情况，这样就需要综合考虑他们两，最常见的就是F-Score<br>也就是P-R的调和平均。<br><img src="https://img-blog.csdnimg.cn/20200318193333616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200318193343526.png" alt="在这里插入图片描述"></p>
<h3 id="4、ROC与AUC"><a href="#4、ROC与AUC" class="headerlink" title="4、ROC与AUC"></a>4、ROC与AUC</h3><p>对于模型的评估一般都是设定一个阈值，大于阈值的为正样本，小于的为负样本。通常在模型训练出来的值若是按概率进行排序的话（[0，1]之间），我们通常会选择0.5作为最初的阈值，其他的与之进行比较。</p>
<p>分类过程就是在一个排序中找到一个“截断点”（cut point）将样本一分为二。</p>
<p>根据不同的任务我们会采取不同的策略。像是如果我们的任务更重视“查准率”，那么我们的阈值（threshold）就需要选择较为靠前的位置。若是一个非常重视“查全率”的任务，像是对于传染疾病的筛查，我们就需要将阈值放在比较靠后一点的位置上。</p>
<h4 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h4><p>所以ROC和P-R的原理之本是一样的，只不过用来比较的值有所不同。ROC用来比较的值是FPR,TPR。<br><img src="https://img-blog.csdnimg.cn/20200318195739703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>然后我们就会得到一个类似P-R的曲线。评估方式也和P-R曲线相同。<br><img src="https://img-blog.csdnimg.cn/20200318195801930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>ps.当阈值（threshold）取值越多，ROC曲线越平滑。</p>
<h4 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h4><p>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。</p>
<p>AUC到底是什么呢：首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。</p>
<h4 id="为什么使用ROC曲线"><a href="#为什么使用ROC曲线" class="headerlink" title="为什么使用ROC曲线"></a>为什么使用ROC曲线</h4><p>ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。因为在实际应用中的数据集很多情况下都是不平衡的，正样本会远远大于负样本，疑惑反之。二P-R曲线在面对不平衡的的数据集时的表现就不啊太好。<br><img src="https://img-blog.csdnimg.cn/20200318201437796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在上图中，(a)和( c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，( c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。</p>
<p>不用担心这些自己要怎么算，怎么找出最好的阈值。R/Python中都有套件，可以帮我们计算不同阈值的得分，然后标出最高分的阈值的值。</p>
<hr>
<p>引用连接：</p>
<p>(Fawcett, 2006)，Fawcett, T. (2006). An introduction to ROC analysis. Pattern recognition letters, 27(8), 861-874.<br><a href="https://my.oschina.net/liangtee/blog/340317" target="_blank" rel="noopener">https://my.oschina.net/liangtee/blog/340317</a><br><a href="https://blog.csdn.net/fzp95/article/details/86491014" target="_blank" rel="noopener">https://blog.csdn.net/fzp95/article/details/86491014</a><br>周志华《机器学习》</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
      </tags>
  </entry>
  <entry>
    <title>R｜数据的预处理：数据集的合并</title>
    <url>/R%EF%BD%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<p>合并数据集<br>很多情况下，数据集都不会只有一个文件，但是为了方便后续的处理和分析，在预处理的阶段就会将各个数据集进行合并。<br><img src="https://img-blog.csdnimg.cn/20200227104344935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreWV4eA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="1-Inner-Join内连接"><a href="#1-Inner-Join内连接" class="headerlink" title="1. Inner Join内连接"></a>1. Inner Join内连接</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 数据集A</span><br><span class="line">A = data.frame(id=c(1,3,5), A_val=c(&apos;a&apos;,&apos;x&apos;,&apos;c&apos;)) </span><br><span class="line">   id A_val </span><br><span class="line">1  1  a</span><br><span class="line">2  3 x</span><br><span class="line">3  5 c</span><br><span class="line"># 数据集B</span><br><span class="line">B = data.frame(id=c(3,5,6), B_val=c(&apos;x&apos;,&apos;y&apos;,&apos;z&apos;)) </span><br><span class="line">   id B_val </span><br><span class="line">1  3  x</span><br><span class="line">2  5  y</span><br><span class="line">3  6  z</span><br><span class="line"># 函数merge( )通过“id”取交集，所以只剩下 id为3 5的</span><br><span class="line">A_B = merge(x=A, y=B, by.x=&apos;id&apos;, by.y=&apos;id&apos;) A_B</span><br><span class="line">   id A_val B_val </span><br><span class="line">1  3    x     x </span><br><span class="line">2  5    c     y</span><br></pre></td></tr></table></figure>

<h4 id="2-Out-Join-外连接"><a href="#2-Out-Join-外连接" class="headerlink" title="2. Out Join 外连接"></a>2. Out Join 外连接</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 通过“id”将B外接导A上</span><br><span class="line">left_A_B = merge(x=A, y=B, by.x=&apos;id&apos;, by.y=&apos;id&apos;, all.x=T)</span><br><span class="line">left_A_B # A left join B</span><br><span class="line">    id A_val B_val </span><br><span class="line">1   1    a    &lt;NA&gt;</span><br><span class="line">2   3    x     x </span><br><span class="line">3   5    c     y</span><br><span class="line"># 通过“id”将A外接导B上</span><br><span class="line">right_A_B = merge(x=A, y=B, by.x=&apos;id&apos;, by.y=&apos;id&apos;, all.y=T) </span><br><span class="line">right_A_B # A right join B</span><br><span class="line">    id A_val B_val </span><br><span class="line">1   3    x     x </span><br><span class="line">2   5    c     y</span><br><span class="line">3   6   &lt;NA&gt;   z</span><br><span class="line"># 通过“id”将AB都保留</span><br><span class="line">full_A_B = merge(x=A, y=B, by.x=&apos;id&apos;, by.y=&apos;id&apos;, all=T) </span><br><span class="line">full_A_B # A full outer join B</span><br><span class="line">    id A_val B_val </span><br><span class="line">1   1    a    &lt;NA&gt; </span><br><span class="line">2   3    x     x </span><br><span class="line">3   5    c     y</span><br><span class="line">4   6   &lt;NA&gt;   z</span><br></pre></td></tr></table></figure>

<h4 id="3-Concatenation"><a href="#3-Concatenation" class="headerlink" title="3. Concatenation"></a>3. Concatenation</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将A B数据集columns的名字改成统一的</span><br><span class="line">colnames(A) = colnames(B) = c(&apos;id&apos;,&apos;val&apos;);</span><br><span class="line"># 然后用rbind()将A B两组数据集通过row合并，B接在A的下面(需要相同的column)</span><br><span class="line">rbind(A,B) # Concatenate vertically</span><br><span class="line">  id val</span><br><span class="line">1 1   a</span><br><span class="line">2 3   x</span><br><span class="line">3 5   c </span><br><span class="line">4 3   x </span><br><span class="line">5 5   y </span><br><span class="line">6 6   z</span><br><span class="line"></span><br><span class="line"># 将A B数据集通过column合并，B接在A的后面</span><br><span class="line">cbind(A,B) # Concatenate horizontally </span><br><span class="line">    id val  id val</span><br><span class="line">1   1   a   3   x </span><br><span class="line">2   3   x   5   y </span><br><span class="line">3   5   c   6   z</span><br></pre></td></tr></table></figure>

<h4 id="4-Set-Operation交集"><a href="#4-Set-Operation交集" class="headerlink" title="4. Set Operation交集"></a>4. Set Operation交集</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># A intersect B</span><br><span class="line"> subset(A, (A$id %in% B$id &amp; A$val %in% B$val))</span><br><span class="line">    id val</span><br><span class="line">2   3   x</span><br><span class="line"># A except B</span><br><span class="line"> subset(A, ! (A$id %in% B$id &amp; A$val %in% B$val))</span><br><span class="line">    id val</span><br><span class="line">1   1   a</span><br><span class="line">3   5   c</span><br><span class="line"># B except A</span><br><span class="line"> subset(B, ! (B$id %in% A$id &amp; B$val %in% A$val))</span><br><span class="line">    id val </span><br><span class="line">2   5   y </span><br><span class="line">3   6   z</span><br></pre></td></tr></table></figure>

<h4 id="5-用dplyr包来查找交集"><a href="#5-用dplyr包来查找交集" class="headerlink" title="5. 用dplyr包来查找交集"></a>5. 用dplyr包来查找交集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># More efficient row/column binding</span></span><br><span class="line">dplyr::bind_rows(A, B); dplyr::bind_cols(A, B)</span><br><span class="line"><span class="comment"># A intersect/union B</span></span><br><span class="line">dplyr::intersect(A, B); dplyr::union(A, B);</span><br><span class="line"><span class="comment"># A except B; B except A</span></span><br><span class="line">dplyr::setdiff(A, B); dplyr::setdiff(B, A)</span><br><span class="line"><span class="comment"># Observation-level set comparison</span></span><br><span class="line">dplyr::setequal(A, B)</span><br></pre></td></tr></table></figure>

<h4 id="6-用SQL语法来合并数据集（sqldf包）"><a href="#6-用SQL语法来合并数据集（sqldf包）" class="headerlink" title="6. 用SQL语法来合并数据集（sqldf包）"></a>6. 用SQL语法来合并数据集（sqldf包）</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">library(sqldf)</span><br><span class="line"><span class="comment"># Inner join</span></span><br><span class="line">sqldf('<span class="keyword">select</span> A.id, A.val <span class="keyword">as</span> A_val, B.val <span class="keyword">as</span> B_val <span class="keyword">from</span> A <span class="keyword">inner</span> <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id<span class="string">')</span></span><br><span class="line"><span class="string"># Left outer join</span></span><br><span class="line"><span class="string">sqldf('</span><span class="keyword">select</span> A.id, A.val <span class="keyword">as</span> A_val, B.val <span class="keyword">as</span> B_val <span class="keyword">from</span> A <span class="keyword">left</span> <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id<span class="string">')</span></span><br><span class="line"><span class="string"># Union</span></span><br><span class="line"><span class="string">sqldf('</span><span class="keyword">select</span> * <span class="keyword">from</span> A <span class="keyword">union</span> <span class="keyword">select</span> * <span class="keyword">from</span> B<span class="string">')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sqldf('</span><span class="keyword">select</span> * <span class="keyword">from</span> A <span class="keyword">except</span> <span class="keyword">select</span> * <span class="keyword">from</span> B<span class="string">')</span></span><br><span class="line"><span class="string">#     id val </span></span><br><span class="line"><span class="string"># 1   1   a</span></span><br><span class="line"><span class="string"># 2   5   c</span></span><br><span class="line"><span class="string"> sqldf('</span><span class="keyword">select</span> * <span class="keyword">from</span> A <span class="keyword">intersect</span> <span class="keyword">select</span> * <span class="keyword">from</span> B<span class="string">')</span></span><br><span class="line"><span class="string">#     id val </span></span><br><span class="line"><span class="string"># 1   3   x</span></span><br><span class="line"><span class="string">sqldf('</span><span class="keyword">select</span> * <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> * <span class="keyword">from</span> A <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> B) <span class="keyword">where</span> <span class="keyword">id</span> &gt; <span class="number">5</span><span class="string">')</span></span><br><span class="line"><span class="string">#     id val </span></span><br><span class="line"><span class="string"># 1   6   z</span></span><br><span class="line"><span class="string">mtcars = data.frame(mtcars);</span></span><br><span class="line"><span class="string">sqldf('</span><span class="keyword">select</span> row_names, mpg, cyl, wt <span class="keyword">from</span> mtcars</span><br><span class="line"><span class="keyword">where</span> row_names <span class="keyword">like</span> <span class="string">"%Toyota%"</span> <span class="string">', row.names=T)</span></span><br><span class="line"><span class="string">#                 mpg cyl wt </span></span><br><span class="line"><span class="string"># Toyota Corolla 33.9 4 1.835 </span></span><br><span class="line"><span class="string"># Toyota Corona 21.5 4 2.465</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>details</tag>
      </tags>
  </entry>
  <entry>
    <title>R｜资料的预处理：删减不需要的资料</title>
    <url>/R%EF%BD%9C%E8%B3%87%E6%96%99%E7%9A%84%E9%A0%90%E8%99%95%E7%90%86%EF%BC%9A%E5%88%AA%E6%B8%9B%E4%B8%8D%E9%9C%80%E8%A6%81%E7%9A%84%E8%B3%87%E6%96%99/</url>
    <content><![CDATA[<p>R的资料处理很常用的package“dplyr” 里面select()常被用来做资料的删减</p>
<h1 id="select-的用法"><a href="#select-的用法" class="headerlink" title="select() 的用法"></a>select() 的用法</h1><p>以mtcars资料集为例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head(mtcars,3)</span><br><span class="line">#                    mpg cyl disp  hp drat    wt  qsec vs am gear carb</span><br><span class="line"># Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4</span><br><span class="line"># Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4</span><br><span class="line"># Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1</span><br></pre></td></tr></table></figure>

<p>删减不需要的资料栏位<br>假设不需要用到 mpy，hp这两个栏位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtcars1=select(mtcars,-c(&quot;mpg&quot;,&quot;hp&quot;))</span><br><span class="line">head(mtcars1,3)</span><br><span class="line">#                   cyl disp drat    wt  qsec vs am gear carb</span><br><span class="line"># Mazda RX4           6  160 3.90 2.620 16.46  0  1    4    4</span><br><span class="line"># Mazda RX4 Wag       6  160 3.90 2.875 17.02  0  1    4    4</span><br><span class="line"># Datsun 710          4  108 3.85 2.320 18.61  1  1    4    1</span><br></pre></td></tr></table></figure>

<p>假设需要mpg到wt的资料,但是不需要cyl和hp这两个栏位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtcars2=select(mtcars,mpg:wt,-c(&quot;cyl&quot;,&quot;hp&quot;))</span><br><span class="line">head(mtcars2,3)</span><br><span class="line">#                mpg disp drat    wt</span><br><span class="line"># Mazda RX4     21.0  160 3.90 2.620</span><br><span class="line"># Mazda RX4 Wag 21.0  160 3.90 2.875</span><br><span class="line"># Datsun 710    22.8  108 3.85 2.320</span><br></pre></td></tr></table></figure>

<h1 id="select-的子函数"><a href="#select-的子函数" class="headerlink" title="select() 的子函数"></a>select() 的子函数</h1><p>select 还有很多非常好用的子函数</p>
<ul>
<li>starts_with()；</li>
<li>ends_with()；</li>
<li>contains()；</li>
<li>matches()；</li>
<li>num_range()；</li>
<li>one_of()；</li>
<li>everything()</li>
</ul>
<p>找出栏位中所有以“c”开头的columns</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtcars3 = select(mtcars,starts_with(&quot;c&quot;))</span><br><span class="line">head(mtcars3,3)</span><br><span class="line">#               cyl carb</span><br><span class="line"># Mazda RX4       6    4</span><br><span class="line"># Mazda RX4 Wag   6    4</span><br><span class="line"># Datsun 710      4    1</span><br></pre></td></tr></table></figure>

<p>删掉栏位名称中有“ar”的columns</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtcars4 = select(mtcars,-contains(&quot;ar&quot;))</span><br><span class="line">head(mtcars4,3)</span><br><span class="line"># 原始           mpg cyl disp  hp drat    wt  qsec vs am gear carb</span><br><span class="line">#                mpg cyl disp  hp drat    wt  qsec vs am</span><br><span class="line"># Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1</span><br><span class="line"># Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1</span><br><span class="line"># Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1</span><br></pre></td></tr></table></figure>

<p>可以看出“ar”是一整个的字符串，所以字母顺序需要注意，有“ra”就被保留下来了</p>
<p>其他的几个子函数的用法也类似starts_with()</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>details</tag>
        <tag>dplyr</tag>
      </tags>
  </entry>
  <entry>
    <title>R｜数据中缺失值NA的处理</title>
    <url>/R%EF%BD%9C%E6%95%B8%E6%93%9A%E4%B8%AD%E7%BC%BA%E5%A4%B1%E5%80%BC-NA-%E7%9A%84%E8%99%95%E7%90%86/</url>
    <content><![CDATA[<p>训练一个机器学习模型，其实大量的时间是花在资料的预处理和探索性资料分析上。尤其是实际中遇到的data都不会太干净，所以花较长的时间来做数据的预处理是很有必要的。</p>
<p>首先来建立一个简单的数据集</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(mice)</span><br><span class="line">name = c(&quot;Andy&quot;,&quot;Helly&quot;,&quot;Ann&quot;,&quot;Ketay&quot;,&quot;Wang&quot;,&quot;Liu&quot;)</span><br><span class="line">country = c(&quot;UK&quot;,&quot;US&quot;,&quot;US&quot;,&quot;US&quot;,&quot;CH&quot;,&quot;CH&quot;)</span><br><span class="line">gender = c(&quot;male&quot;,&quot;female&quot;,NA,&quot;female&quot;,&quot;male&quot;,NA)</span><br><span class="line">age = c(22,19,26,31,45,32)</span><br><span class="line">income = c(12,NA,55,77,32,NA)</span><br><span class="line"></span><br><span class="line">data = data.frame(name,country,gender,age,income)</span><br><span class="line">data</span><br><span class="line"># name country gender age income</span><br><span class="line"># 1  Andy      UK   male  22     12</span><br><span class="line"># 2 Helly      US female  19     NA</span><br><span class="line"># 3   Ann      US   &lt;NA&gt;  26     55</span><br><span class="line"># 4 Ketay      US female  31     77</span><br><span class="line"># 5  Wang      CH   male  45     32</span><br><span class="line"># 6   Liu      CH   &lt;NA&gt;  32     NA</span><br></pre></td></tr></table></figure>

<h2 id="查找NA"><a href="#查找NA" class="headerlink" title="查找NA"></a>查找NA</h2><ul>
<li><p>存在NA的rows</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">complete.cases(data) # 当一笔资料是完整的，回传TRUE；当一笔资料有遗漏值，回传FALSE</span><br><span class="line">#[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE</span><br></pre></td></tr></table></figure>
</li>
<li><p>查找缺失值的位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">which(is.na(data))  #返回缺失值的位置</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算数据集中有缺失的资料笔数所占比例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">loss = sum(is.na(data)) #计算资料集中的缺失值总数</span><br><span class="line">have = sum(complete.cases(data)) #统计资料集中完整样本的个数</span><br><span class="line">ratio = loss/(loss+have) #计算缺失值资料的比重</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="填补NA"><a href="#填补NA" class="headerlink" title="填补NA"></a>填补NA</h2><ol>
<li><p>na.omit() 可以删除所有含有缺失资料的row</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data1 = na.omit(data)</span><br><span class="line">data1</span><br><span class="line"># name country gender age income</span><br><span class="line"># 1  Andy      UK   male  22     12</span><br><span class="line"># 4 Ketay      US female  31     77</span><br><span class="line"># 5  Wang      CH   male  45     32</span><br></pre></td></tr></table></figure>
</li>
<li><p>最高频率来填补缺失值.尝试找到这些缺失值最可能的值。</p>
<blockquote>
<p>对于变数分布近似正态分布时可以选用平均值；偏态分布一般采用中位数代表资料中心趋势的指标。</p>
</blockquote>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mean_income = mean(data$income)</span><br><span class="line">data2 = data</span><br><span class="line">#一些函式计算时拥有na.rm=TRUE，可以在计算以前移除缺失值并使用剩余值进行计算</span><br><span class="line">data2[is.na(data2$income),&quot;income&quot;] = mean(data2$income,na.rm = T) #用平均数补充income的缺失值</span><br><span class="line">data2</span><br><span class="line"># name country gender age income</span><br><span class="line"># 1  Andy      UK   male  22     12</span><br><span class="line"># 2 Helly      US female  19     44</span><br><span class="line"># 3   Ann      US   &lt;NA&gt;  26     55</span><br><span class="line"># 4 Ketay      US female  31     77</span><br><span class="line"># 5  Wang      CH   male  45     32</span><br><span class="line"># 6   Liu      CH   &lt;NA&gt;  32     44</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>函式centralImputation()可以用资料的中心趋势值来填补资料集的所有缺失值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data3 = data</span><br><span class="line">data3$income &lt;- centralImputation(data3$income)</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过变数的相关关系填补缺失值 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#函式cor()的功能是产生变数之间的相关值矩阵,引数use = &quot;complete.obs&quot;可以忽略含有NA的记录</span><br><span class="line">cor(data[,c(&quot;age&quot;,&quot;income&quot;)],use = &quot;complete.obs&quot;) # =cor(data[,4:5],use = &quot;complete.obs&quot;)</span><br><span class="line"># age     income</span><br><span class="line"># age    1.00000000 0.07670152</span><br><span class="line"># income 0.07670152 1.00000000</span><br><span class="line">lm(age~income,data = data) </span><br><span class="line">#函式lm()可以用来获取线性模型</span><br><span class="line">#可以使用上述线性关系计算变数的缺失值</span><br></pre></td></tr></table></figure>
</li>
<li><p>用K-Nearest Neighbours填补遗漏值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">require(DMwR)</span><br><span class="line">imputeData &lt;- knnImputation(data)</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>details</tag>
      </tags>
  </entry>
  <entry>
    <title>R｜建模范例分析</title>
    <url>/R%EF%BD%9C%E5%BB%BA%E6%A8%A1%E7%AF%84%E4%BE%8B%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h1 id="R建模范例分析"><a href="#R建模范例分析" class="headerlink" title="R建模范例分析"></a>R建模范例分析</h1><p>Regression analysis is widely used in statisitic, always  analysis the correlation among  different variables. </p>
<blockquote>
<p>这是一个非常简单资料处理和建模的例子，主要是为了熟悉整个资料分析和建模的流程。其中有很多细节可以继续展开。</p>
</blockquote>
<h2 id="读取资料"><a href="#读取资料" class="headerlink" title="读取资料"></a>读取资料</h2><p>大部分读取资料，读取excel档一本需要转成CSV档来读取。<br>这里我们以 iris.csv 这个 CSV 档案作为范例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 读取 iris.csv</span><br><span class="line">my.iris.df &lt;- read.csv(&quot;iris.csv&quot;)</span><br></pre></td></tr></table></figure>

<p>如果自己的csv档没有放在目前的目录中，可以使用绝对路径来指定：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用绝对路径</span><br><span class="line">my.iris.df &lt;- read.csv(&quot;D:\\iris.csv&quot;)</span><br></pre></td></tr></table></figure>

<p><strong>双反斜线（\）</strong>在R中属于特殊的跳脱字元，所以在撰写绝对路径的时候，凡是要输入反斜线的地方，都要改为双反斜线。</p>
<p>读取资料完成后，为了方便查看大体量的资料可以用head（）来查看前面几行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看 my.iris.df 的资料的前面20rows</span><br><span class="line">head(my.iris.df, 20)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>将csv表格读进R之后个栏位若有空白或是特殊字元，会被自动替代为（.），它不会影响到资料，只是在变数指定的时候要使用新的名字</p>
</blockquote>
<h2 id="资料的预处理"><a href="#资料的预处理" class="headerlink" title="资料的预处理"></a>资料的预处理</h2><p>读进去之后我们就需要更详细的了解资料的内部结构：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看 my.iris.df 内部结构</span><br><span class="line">str(my.iris.df)</span><br></pre></td></tr></table></figure>

<p>str输出的一个row就是一个变数的栏位，标示了变数名称与类型，后面接的是实际的资料笔数。</p>
<p>检查完资料的类型没有什么问题之后，我们需要处理资料中残留的一些不干净的数据以及缺失值NA。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检查是否有 NA 的资料</span><br><span class="line">my.iris.df[!complete.cases(my.iris.df),]</span><br></pre></td></tr></table></figure>

<p>如果资料是完整的就会返回一个空的dataframe</p>
<h2 id="分析资料与绘图"><a href="#分析资料与绘图" class="headerlink" title="分析资料与绘图"></a>分析资料与绘图</h2><p>通常在开始分析资料之前，会先用 <strong>summary</strong> 看一下各个变数的基本统计量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看基本统计量</span><br><span class="line">summary(my.iris.df)</span><br></pre></td></tr></table></figure>

<p>当然光看数字肯定是非常不直观的，绘图会是一个查看各组数据相关性的好方法。这里我们示范使用** ggplot2 与 GGally** 套件来画图的方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 载入 ggplot2 与 GGally 套件</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(GGally)</span><br></pre></td></tr></table></figure>

<p>使用 ggpairs 可画出很漂亮的资料<strong>分布矩阵图</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 绘制资料分布矩阵图</span><br><span class="line">ggpairs(my.iris.df)</span><br></pre></td></tr></table></figure>

<p>如果只想画出简单的 <strong>XY 散布图</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 画出 XY 散布图</span><br><span class="line">require(ggplot2)</span><br><span class="line">qplot(x = Petal.Length,</span><br><span class="line">      y = Petal.Width,</span><br><span class="line">      data = my.iris.df)</span><br></pre></td></tr></table></figure>

<p>如果想要比较不同的 Species 类别的 XY 散布图，可以用<strong>color</strong>区分出不同的species：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 画出 XY 散布图，依据 Species 上色</span><br><span class="line">require(ggplot2)</span><br><span class="line">qplot(x = Petal.Length,</span><br><span class="line">      y = Petal.Width,</span><br><span class="line">      data = my.iris.df,</span><br><span class="line">      color = Species)</span><br></pre></td></tr></table></figure>

<h2 id="建立回归模型"><a href="#建立回归模型" class="headerlink" title="建立回归模型"></a>建立回归模型</h2><p>建立回归模型可以使用 <strong>lm</strong> 这个函数（lm 代表 linear models），假设我们想要拿 Sepal.Length 作为反应变数（也就是 Y），而 Sepal.Width、Petal.Length 与 Petal.Width 作为解释变数（也就是 X），则可以执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 建立回归模型</span><br><span class="line">iris.lm &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,</span><br><span class="line">              data = my.iris.df)</span><br></pre></td></tr></table></figure>

<p>其中第一个参数放的就是所谓的公式（formula），它用来表示回归模型的一种表示法，中间的** ~ <strong>相当于回归公式的</strong>等号**，左边的 Sepal.Length 就是 Y，而右边放的三个变数则是 X。第二个参数 data 则是用来指定资料来源的 data frame。</p>
<p>建立好回归模型之后，可以使用 summary 查看模型配适的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看模型配适结果</span><br><span class="line">summary(iris.lm)</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,</span><br><span class="line">    data = my.iris.df)</span><br><span class="line">Residuals:</span><br><span class="line">     Min       1Q   Median       3Q      Max</span><br><span class="line">-0.82816 -0.21989  0.01875  0.19709  0.84570</span><br><span class="line">Coefficients:</span><br><span class="line">             Estimate Std. Error t value Pr(&gt;|t|)</span><br><span class="line">(Intercept)   1.85600    0.25078   7.401 9.85e-12 ***</span><br><span class="line">Sepal.Width   0.65084    0.06665   9.765  &lt; 2e-16 ***</span><br><span class="line">Petal.Length  0.70913    0.05672  12.502  &lt; 2e-16 ***</span><br><span class="line">Petal.Width  -0.55648    0.12755  -4.363 2.41e-05 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line">Residual standard error: 0.3145 on 146 degrees of freedom</span><br><span class="line">Multiple R-squared:  0.8586,    Adjusted R-squared:  0.8557</span><br><span class="line">F-statistic: 295.5 on 3 and 146 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>

<p>从这份输出的结果中，我们可以看出各个解释变数的系数，若将整个模型写出来就会像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sepal.Length = 0.65084 * Sepal.Width</span><br><span class="line">             + 0.70913 * Petal.Length</span><br><span class="line">             - 0.55648 * Petal.Width</span><br><span class="line">             + 1.856</span><br></pre></td></tr></table></figure>

<p>报表中的 Pr(&gt;|t|) 就是统计上的 p-value，以这里的值来说，每一个系数都非常显著。<br>R-squared 的值为 0.8586，Adjusted R-squared 的值为 0.8557，表示模型的配适情况不错</p>
<h2 id="模型诊断"><a href="#模型诊断" class="headerlink" title="模型诊断"></a>模型诊断</h2><p>在配适完回归模型之后，接着要进行模型诊断，通常会先画出几张常用来诊断模型的图，这里我们用 ggfortify 这个套件来画图。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(ggfortify)</span><br><span class="line"># 画出模型诊断用的图</span><br><span class="line">autoplot(iris.lm)</span><br></pre></td></tr></table></figure>

<p>在理论上回归模型配适完成后，其<strong>残差值（residual）</strong>必须符合<strong>常态性（normality）</strong>、<strong>独立性（independence）</strong>以及<strong>变异数同质性（homogeneity of variance）</strong>三种假设，所以接下来要用三种检定检查这三个条件是否符合。</p>
<h3 id="残差独立性检定"><a href="#残差独立性检定" class="headerlink" title="残差独立性检定"></a>残差独立性检定</h3><p>独立性检定要使用到 car 这个套件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 残差独立性检定</span><br><span class="line">require(car)</span><br><span class="line">durbinWatsonTest(iris.lm)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lag Autocorrelation D-W Statistic p-value</span><br><span class="line">  1     -0.03992126      2.060382   0.822</span><br><span class="line">Alternative hypothesis: rho != 0</span><br></pre></td></tr></table></figure>

<p>残差独立性检定的 p-value 也非常高，所以也不拒绝虚无假设，亦即残差值的独立性假设是合理的。</p>
<h3 id="残差变异数同质性检定"><a href="#残差变异数同质性检定" class="headerlink" title="残差变异数同质性检定"></a>残差变异数同质性检定</h3><p>残差的变异数同质性检定可以使用 car 套件所提供的 ncvTest（）函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 残差变异数同质性检定</span><br><span class="line">require(car)</span><br><span class="line">ncvTest(iris.lm)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Non-constant Variance Score Test </span><br><span class="line">Variance formula: ~ fitted.values </span><br><span class="line">Chisquare = 4.448612    Df = 1     p = 0.03492962</span><br></pre></td></tr></table></figure>

<p>残差变异数同质性检定的 p-value 稍微偏小，以一般 95% 的信赖水准来说，是拒绝虚无假设的，也就是说残差的变异数没有符合同质性的假设，但是因为这个 p-value 并没有非常小，所以证据并不是非常明确。</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>在回归模型建立好之后，就可以利用这个模型来预测新的资料，假设我们收到一些新的观测值（解释变数 X）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 新观测值</span><br><span class="line">new.iris &lt;- data.frame(Sepal.Width=3.1, Petal.Length=1.6, Petal.Width=0.3)</span><br><span class="line">new.iris</span><br></pre></td></tr></table></figure>

<p>若要使用回归模型预测新的反应变数，可以使用 predict 函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 预测资料</span><br><span class="line">predict(iris.lm, new.iris)</span><br></pre></td></tr></table></figure>

<pre><code>1 </code></pre><p>4.841259<br>预测出来的 Sepal.Length 值就是 4.841259。</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Pwc性格测试</title>
    <url>/Pwc%E6%80%A7%E6%A0%BC%E6%B8%AC%E8%A9%A6/</url>
    <content><![CDATA[<h1 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h1><p>因为想找个分析类 winter internship 所以投了pwc，很快就接到了笔试的邮件。今年的笔试其实还挺有意思的，从原有的单纯智商测试题，变为了互动性比较强的游戏，不过感觉内容其实变化不大，不过趣味性确实比较强。游戏通关后会有一个性格测试的报告，其实不管之后会不会接到 interview 的面试，这份报告，对于未来的成长和自我反思还是很有参考价值的。</p>
<h1 id="报告的内容"><a href="#报告的内容" class="headerlink" title="报告的内容"></a>报告的内容</h1><h2 id="个人风格"><a href="#个人风格" class="headerlink" title="个人风格"></a>个人风格</h2><p>相对于比较组，你的反应表明在解读情绪和面部表情方面，你的准确度趋向于非常高。</p>
<h2 id="认知"><a href="#认知" class="headerlink" title="认知"></a>认知</h2><p>你倾向于加工信息量较小的信息，对于处理信息量较大的信息你的自信程度会比较低<br>加工信息的速度属于平均水平</p>
<h2 id="驱动力"><a href="#驱动力" class="headerlink" title="驱动力"></a>驱动力</h2><p>（这个结果很有意思，让我有点意想不到）</p>
<ul>
<li>相对于比较组，你的反应表明你相当有可能需要<strong>花较长的时间来从挫折中恢复</strong>。同时在<strong>逆境之下</strong>，你在相当程度上更有可能<strong>难以保持对目标的专注</strong>。</li>
<li>相对于比较组，你的注意力会平均地受你的价值观、目标、想法、感觉和生理状态影响。</li>
<li>相对于比较组，你的反应表明你在很大程度上倾向于更愿意出力来完成所想要达成的结果，以获得内在或外在的奖励。</li>
</ul>
<blockquote>
<p>确实在面对逆境的时候我很容易把自己的生活搞的混乱不堪，然后花很长的时间从这片泥沼中慢慢爬起来。明白也许逆境是生活的常态，学会勇敢的面对挫折，不要把时间浪费在自怜自艾中，是我要努力做到。<br>我有点没有想到我会是一个受奖励驱动力很高的人</p>
</blockquote>
<h2 id="人际交往风格"><a href="#人际交往风格" class="headerlink" title="人际交往风格"></a>人际交往风格</h2><ul>
<li>相对于比较组，你的反应表明你在很大程度上更倾向于寻求社交互动并会因其而感到充满活力。</li>
<li>相对于比较组，你的反应表明你在非常大的程度上倾向于表现得更自信、决断和有把握。</li>
<li>你的反应表明，在管理自己的行为以对社交线索作出回应方面，你的倾向程度与比较组中的其他大部分人相同。</li>
<li>相对于比较组，你的反应表明你在相当程度上倾向于采取与其他人的需求相符，而不是与自己的需求相符的行动。</li>
</ul>
<h2 id="思维风格"><a href="#思维风格" class="headerlink" title="思维风格"></a>思维风格</h2><ul>
<li><strong>相对于比较组，你的反应表明在做决策时，你略微地倾向于将注意力集中在直接成果</strong>。<br>（直白的说，我就我可能更注重短期利益，没很好的长期收益的概念）</li>
<li>在对世界和未来拥有积极看法上，你的倾向程度与比较组中的其他大部分人相同。在预计行动最后会产生更为有利或不太有利的结果上，你的预期较为合理。</li>
<li>你的结果表明，在做涉及到一定程度⻛险的决策前，你在会仔细考虑⻛险收益比上的倾向程度与比较组中的其他大部分人相同。</li>
<li><strong>相对于比较组，你的反应表明你在做决策时倾向于需要程度稍微较大的确定性。当未来较为容易预测时，你也会感到较为自在</strong>。</li>
<li>解决问题时，你在尝试新颖和实验性的方法上的倾向程度与比较组中的其他大部分人相同。</li>
<li>你的反应表明，在理性决策⻛格方面，你的倾向程度与比较组中的其他大部分人相同。在做决策时，你的思维可能会显得周密、客观和具有批判性。</li>
<li>相对于比较组，在回应前仔细考虑自己的行动方面，你的倾向程度与比较组中的其他大部分人相同。</li>
<li>在偏爱多样性、经常性的变化，以及喜好尝试新事物上，你的可能性与比较组中的其他大部分人相同。</li>
<li>在创造性地思考和拥有自由畅通的思路及抽象性思路上，你的倾向程度与比较组中的其他大部分人相同。</li>
</ul>
<blockquote>
<p>有时比较短视的想法，可能会限制更好的长期发展。可以培养一些短期不会有太大效果，但对于长期的人生会有比较好收益的习惯。</p>
</blockquote>
]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>reflection</tag>
      </tags>
  </entry>
  <entry>
    <title>Books |《数据挖掘与数据化运营实战》</title>
    <url>/Books%EF%BD%9C%E3%80%8A%E6%95%B8%E6%93%9A%E6%8C%96%E6%8E%98%E8%88%87%E6%95%B8%E6%93%9A%E5%8C%96%E9%81%8B%E7%87%9F%E5%AF%A6%E6%88%B0%E3%80%8B/</url>
    <content><![CDATA[<p>   一直对数据分析很感兴趣，但在学校学习的都比较简单，又碍于自己的懒惰，一直没有深入了解。暑假借着报名参加了一个数据分析的比赛（虽然在初赛就折戟了😂其实一点也不意外）让自己比较有动力继续更深入的学习一些数据处理的方法，当然还有如何用机器学习的方式去实现进一步的预测，以及决策。</p>
<p>   也是在图书馆闲逛的时候，意外发现的这本《数据挖掘与数据化运营实战》，相较于其他的一些数据挖掘和机器学习的书籍，这本书更偏向于实战应用。它没有过多的数学推导，比较测重于数据分析整体流程的一个介绍，对于一个初学者来说是一部很好的入门书籍。使初学者更加清楚的知道数据分析的应用情境，数据挖掘有哪些流程步骤，在每个步骤中我们该如何选择最好的方法来达到期望的效果等等。总之对于一个数据和编程小白来说这是一本非常友好的入门书籍，看后会对数据挖掘有一个比较宏观的认识，当然想要了解算法原理之类的一些核心技能这本书就不太够用了，这些就需要更深入的一些书籍去了解了。</p>
<p>我的阅读顺序可能比较奇怪，所以Reading notes也是各个章节穿插着纪录的。</p>
<h1 id="第六章-数据挖掘项目完整应用案例演示"><a href="#第六章-数据挖掘项目完整应用案例演示" class="headerlink" title="第六章 数据挖掘项目完整应用案例演示"></a>第六章 数据挖掘项目完整应用案例演示</h1><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>几个希望传递的重点：</p>
<ul>
<li>数据挖掘是有一定的基本流程和顺序的，按照流程进行挖掘是数据挖掘分析严谨性的体现。</li>
<li>数据挖掘只是数据运营的一部分，没有落地应用的数据挖掘严格意义上还不能算是“完成”。</li>
<li>落地应用中的运营方案对模型的应用效果影响极大，所以数据分析师不仅仅要熟悉数据分析和模型的搭建，还要熟悉与运营相关的业务。</li>
</ul>
<h2 id="项目背景和业务分析需求的提出"><a href="#项目背景和业务分析需求的提出" class="headerlink" title="项目背景和业务分析需求的提出"></a>项目背景和业务分析需求的提出</h2><ul>
<li>背景：某互联网公司“免费用户运营团队”，需要不断将免费用户提升为付费用户，来从电子商务中获取更大的利益。</li>
<li>困境：高活跃度的群体付费转化率最高，但是高活跃度用户的流失比较大，有相当比例的高活跃度用户会快速的跌落到中低活跃度中。</li>
</ul>
<h2 id="数据分析师参与需求讨论"><a href="#数据分析师参与需求讨论" class="headerlink" title="数据分析师参与需求讨论"></a>数据分析师参与需求讨论</h2><p>分析师与运营方进行了需求的讨论。<br>讨论的目的主要是：</p>
<ul>
<li>针对需求收集相关的背景数据和指标，与业务方一起熟悉背景中的相关业务逻辑，并收集业务方对需求的相关建议和看法。这些信息对需求的确认和思路的规划乃至后期的分析都至关重要。</li>
<li>从数据的分析的角度评估业务分析是否合理。某些情况下，某些分析就是“伪命题”</li>
</ul>
<h2 id="制订需求分析框架和分析计画"><a href="#制订需求分析框架和分析计画" class="headerlink" title="制订需求分析框架和分析计画"></a>制订需求分析框架和分析计画</h2><p>初步了解了背景，要制订初步的分析计画和分析框架。<br>分析框架的主要内容：</p>
<ul>
<li><strong>分析需求转化成数据分析项目中目标变量的定义</strong>。具体到之前案例，定义“高度活跃免费用户的流失”：[在某个时间点“A点”用户是满足高活跃用户标准要求的（这时是属于高活跃用户群体的），随后过“A点”7天（这个7天也是根据运营的时间节奏来订出的），该高活跃用户跌入至中低活跃用户中，并在过“A点”14天，即两周之后仍然没有回到高活跃的标准。 ]这只是初步的定义，随着后期进行数据抽取，并与业务方讨论后，有跟深入的分析后，上述的定义可以被修改和完善，修改和完善的终极目的是为了数据分析和挖掘工作能最有效的达到预期效果，并提升业务工作效率。</li>
<li><strong>分析思路的大致描述</strong>。具体到上述方案，通过搭建分类模型来比较准确的锁定有可能流失的用户群体。</li>
<li><strong>分析样本的数据抽取规则</strong>。数据抽取规则因项目而定，基本上是根据上面的目标变量的定义，选择一个合适的时间窗口，然后抽取一定的样本数据。</li>
<li><strong>潜在分析变量（模型输入变量）的大致圈定和罗列</strong>。经过前期的分析和讨论，分析师已经确定了大致圈定的相关变量（从业务经验判断和以往分析中得来），上述案例中整理出大约63个原始变量。罗列出这些似乎对目标变量的预测有意义的相关变量。</li>
<li><strong>分析过程中的项目风险思考和主要应对策略</strong>。具体到上述案例，项目风险思考主要包括模型效果不好的可能性，即有可能分类模型的思路被证明是不好的，也有可能是模型效果不好，或者准确度不高，或者模型不稳定。是否有相应的分析对策来部分弥补，若分类模型的思路被证明是行不通的，可退而求其次进行流失用户的群体特征细分，或者重新定义流失用户等。</li>
<li><strong>项目落地应用价值分析和展望</strong>。针对上述方案，主要有3个方面：模型投入应用有提前锁定目标群体，使运营方有针对性的开展挽留工作；可以将建模过程中发现的有价值的，最可能影响流失的重要字段和指标选出来提供给运营方，用于制订运营方案和策略的参考和依据；准对影响流失的核心指标和字段，可以提供给相关业务方，开展针对性的策略。</li>
</ul>
<p>书中的项目所给出的一个参考时间表</p>
<blockquote>
<p>分析计画时间表举例：<br>11.5～11.11  数据的抽样和摸底阶段<br>11.12～11.18 数据的前期分析阶段<br>11.19～11.30 建模时间和业务方讨论时间<br>12.1～12.9   模型验证阶段，验证通过，提交分析结果和运营方案建议<br>12.10～12.23 运营方案的落地应用实施<br>12.24～1.8   效果评估和总结，优化方案，落地应用并监控效果</p>
</blockquote>
<h2 id="抽取样本数据，熟悉数据，数据清洗和摸底"><a href="#抽取样本数据，熟悉数据，数据清洗和摸底" class="headerlink" title="抽取样本数据，熟悉数据，数据清洗和摸底"></a>抽取样本数据，熟悉数据，数据清洗和摸底</h2><ul>
<li>本阶段的主要内容是：根据前期的分析和建模思路，以及初步圈定的分析字段（分析变量）编写代码，从数据库中提取分析建模所需的样本数据；通过对样本数据的熟悉和摸底，找到无效数据，脏数据，错误数据等。并且对样本数据中存在的这些明显的数据质量问题进行清洗，剔除，转换，同时视具体业务场景和项目需求，决定是否产生衍生变量，以及怎样衍生等。</li>
<li>针对数据质量的对策：<br>通过对原始样本数据和原始变量的摸底，排查，发现有些变量缺失值高达50%。经过研究发现这些缺失是数据仓库储存过程中的记录缺失，或是由于产品优化后的业务逻辑更改所造成的。这些无法滚回的数据，可以选择直接删除。<ul>
<li>通过输入变量之间的相关性分析，找出潜在共线性问题的相关输入变量，对于高度线性相关的变量只保留一个。</li>
<li>在数据库的数据回滚过程中造成了某些字节的严重不符合逻辑或明显自相矛盾，比如用户最近30天登陆网站次数为0。针对如此不符合逻辑的数据，直接重新回滚数据，直到数据正确为止。</li>
</ul>
</li>
</ul>
<p>经过处理，即删除严重缺失数据，素居仓库重新回滚自相矛盾的数据，对高度相关性的数据部分有取有舍，在本阶段共保留了36个表有意义的字段（变量）合相关数据。 （最开始是63个）<br>（第8章会比较详细的讲到数据清洗）</p>
<h2 id="按计画初步搭建挖掘模型"><a href="#按计画初步搭建挖掘模型" class="headerlink" title="按计画初步搭建挖掘模型"></a>按计画初步搭建挖掘模型</h2><p>对数据进行初步的摸底和清洗之后，就进入初步搭建挖掘模型阶段了。在该阶段，包括3个主要的工作内容:<br>进一步筛选模型的输入变量。最终静如模型的输入变量应遵循“少而精”的总原则。该原则一方面能提高模型的稳定性，利益方面也是为了有效提升模型的预测精准度。关于如何删选模型的输入变量（8.6节，9.3.3节，第十章中会有比较详细的分析。）<br>尝试不同的挖掘算法和分析方法，并比较不同方案的效果，效率和稳定性。关于模型的比较和优化。 7.4节有比较详细的总结。 （这里使用了Neural network；Reg；Tree这三种，还需要有一条Baseline）<br>整理经过模型挑选出来的与目标变量的预测最相关的一系列核心输入变量，将其作为与业务方讨论落地应用时的参考和建议。</p>
<h2 id="与业务方讨论模型的初步结论，提出新的思路和模型优化方案"><a href="#与业务方讨论模型的初步结论，提出新的思路和模型优化方案" class="headerlink" title="与业务方讨论模型的初步结论，提出新的思路和模型优化方案"></a>与业务方讨论模型的初步结论，提出新的思路和模型优化方案</h2><p>在本阶段，需要整理模型的初步报告，结论，以及对主要预测字段（特征）进行提炼，还要通过与业务方沟通和分享，在此基础上讨论出模型的可能优化方向，并对落地应用的方案进行讨论，同时罗列出注意事项。<br>具体针对这个项目而言，除了模型比较之外，还对核心自变量进行了整理提炼，并进行了权重排序。<br>“预测模型的搭建和完善也跟网站分析一样，遵循着‘持续优化，永无止境’”的规律。</p>
<h2 id="按优化方案重新抽取样本并建模，提炼结论并验证模型"><a href="#按优化方案重新抽取样本并建模，提炼结论并验证模型" class="headerlink" title="按优化方案重新抽取样本并建模，提炼结论并验证模型"></a>按优化方案重新抽取样本并建模，提炼结论并验证模型</h2><p>在上述优化方案和新增衍生变量的基础上，重新抽取样本，一方面验证之前的重要猜想；另一方面尝试搭建新的模型提升预测效果。增加新的衍生变量后重复之前的测试，看哪一个模型效果好。这里有提到“初步可以认为，目前的神经网络模型相比于其他模型而言有更好的预测效果，可以更多的有效锁定有流失风险的用户”<br>模型建好后还需要用新的数据来验证模型的稳定性。</p>
<h2 id="完成分析报告和落地应用的建议"><a href="#完成分析报告和落地应用的建议" class="headerlink" title="完成分析报告和落地应用的建议"></a>完成分析报告和落地应用的建议</h2><p>在上述模型优化和验证的基础上，提交给业务方一份详细完整的项目结论和应用建议，包括的内容应该有：<br>模型的预测效果和效率，以及在最新的实际数据中验证模型的结果，即模型稳定性。<br>通过模型整理出来的可以作为营运参考的重要自变量及相应特征，规律。<br>数据分析师根据模型效果和效率数据提出的落地应用的分层建议，以及相应的运营建议，其包括：预测模型打分应用基础上进一步的客户特征分层，相应细分群体运营通道的选择等。</p>
<h2 id="制定具体的落地应用方案和评估方案"><a href="#制定具体的落地应用方案和评估方案" class="headerlink" title="制定具体的落地应用方案和评估方案"></a>制定具体的落地应用方案和评估方案</h2><h2 id="方案落地应用，并跟踪评估"><a href="#方案落地应用，并跟踪评估" class="headerlink" title="方案落地应用，并跟踪评估"></a>方案落地应用，并跟踪评估</h2><h2 id="落地应用方案在实际效果评估后，不断修正完善"><a href="#落地应用方案在实际效果评估后，不断修正完善" class="headerlink" title="落地应用方案在实际效果评估后，不断修正完善"></a>落地应用方案在实际效果评估后，不断修正完善</h2><h2 id="不同运营方案的评估，总结和反馈"><a href="#不同运营方案的评估，总结和反馈" class="headerlink" title="不同运营方案的评估，总结和反馈"></a>不同运营方案的评估，总结和反馈</h2><h2 id="醒目应用后的总结和反思"><a href="#醒目应用后的总结和反思" class="headerlink" title="醒目应用后的总结和反思"></a>醒目应用后的总结和反思</h2><p>“完美的分析结论和模型搭建只是数据化运营万里长城的第一步”，想要模型正真推动业务的效率和效益，模型落地应用的环节更加关键，更加重要，更加复杂。</p>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown 入門</title>
    <url>/Markdown%E5%85%A5%E9%96%80/</url>
    <content><![CDATA[<h1 id="Hexo中Markdown的一些基本語法"><a href="#Hexo中Markdown的一些基本語法" class="headerlink" title="Hexo中Markdown的一些基本語法"></a>Hexo中Markdown的一些基本語法</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><pre><code>分段：兩個回車
換行：回車
標題：# ~ ######，#號的個數表示幾級標題，即表示一級標題到六級標題
強調：**文字** ， __文字__ ， _文字_ ， *文字* ， 文字
引用：&gt; 注意後面緊跟個空格
表格：- 和 | 分割行和列 ， : 控制對其方式</code></pre><h3 id="標題"><a href="#標題" class="headerlink" title="標題"></a>標題</h3><pre><code># 一級標題
## 二級標題
### 三級標題
###### 六級標題</code></pre><p>效果如下：</p>
<h1 id="一級標題"><a href="#一級標題" class="headerlink" title="一級標題"></a>一級標題</h1><h2 id="二級標題"><a href="#二級標題" class="headerlink" title="二級標題"></a>二級標題</h2><h3 id="三級標題"><a href="#三級標題" class="headerlink" title="三級標題"></a>三級標題</h3><h6 id="六級標題"><a href="#六級標題" class="headerlink" title="六級標題"></a>六級標題</h6><h3 id="強調"><a href="#強調" class="headerlink" title="強調"></a>強調</h3><pre><code>**加粗**
*傾斜*
***傾斜加粗***
~~加刪除線~~
`高亮突出背景色`</code></pre><p>效果如下：<br><strong>加粗</strong><br><em>傾斜</em><br><strong><em>傾斜加粗</em></strong><br><del>加刪除線</del><br><code>高亮突出背景色</code></p>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><pre><code>&gt; 這是引用內容
&gt;&gt; 這是引用內容
&gt;&gt;&gt;&gt;&gt;&gt; 這是引用內容</code></pre><p>效果如下：</p>
<blockquote>
<p>這是引用內容</p>
<blockquote>
<p>這是引用內容</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>這是引用內容</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h3 id="分割線"><a href="#分割線" class="headerlink" title="分割線"></a>分割線</h3><p>三個或以上的 -，* 都可以</p>
<pre><code>***
******</code></pre><p>效果如下:</p>
<hr>
<hr>
]]></content>
      <categories>
        <category>Technological</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
